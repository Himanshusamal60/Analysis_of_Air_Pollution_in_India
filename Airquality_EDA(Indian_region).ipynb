{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c94871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4470f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9daf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad78a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Merger\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9075bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Andhra Pradesh': 'AP', 'Arunachal Pradesh': 'AR', 'Assam': 'AS', 'Bihar': 'BR', 'Chhattisgarh': 'CG', 'Chandigarh': 'CH', 'Delhi': 'DL', 'Gujarat': 'GJ', 'Himachal Pradesh': 'HP', 'Haryana': 'HR', 'Jharkhand': 'JH', 'Jammu and Kashmir': 'JK', 'Karnataka': 'KA', 'Kerala': 'KL', 'Maharashtra': 'MH', 'Meghalaya': 'ML', 'Manipur': 'MN', 'Madhya Pradesh': 'MP', 'Mizoram': 'MZ', 'Nagaland': 'NL', 'Odisha': 'OR', 'Punjab': 'PB', 'Puducherry': 'PY', 'Rajasthan': 'RJ', 'Sikkim': 'SK', 'Telangana': 'TG', 'Tamil Nadu': 'TN', 'Tripura': 'TR', 'Uttarakhand': 'UK', 'Uttar Pradesh': 'UP', 'West Bengal': 'WB'}\n"
     ]
    }
   ],
   "source": [
    "def create_dict_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['file_name'] = df['file_name'].str[:2]\n",
    "    result_dict = pd.Series(df['file_name'].values, index=df['state']).to_dict()\n",
    "    return result_dict\n",
    "\n",
    "# Example usage\n",
    "file_path = r'/home/talentum/myproject/datasource/stations_info.csv'\n",
    "state_dict = create_dict_from_csv(file_path)\n",
    "print(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560b4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP', 'AR', 'AS', 'BR', 'CG', 'CH', 'DL', 'GJ', 'HP', 'HR', 'JH', 'JK', 'KA', 'KL', 'MH', 'ML', 'MN', 'MP', 'MZ', 'NL', 'OR', 'PB', 'PY', 'RJ', 'SK', 'TG', 'TN', 'TR', 'UK', 'UP', 'WB']\n"
     ]
    }
   ],
   "source": [
    "print(list(state_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3218be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV files for prefix 'AP' saved as part csv file inside AP folder'\n",
      "Merged CSV files for prefix 'AR' saved as part csv file inside AR folder'\n",
      "Merged CSV files for prefix 'AS' saved as part csv file inside AS folder'\n",
      "Merged CSV files for prefix 'BR' saved as part csv file inside BR folder'\n",
      "Merged CSV files for prefix 'CG' saved as part csv file inside CG folder'\n",
      "Merged CSV files for prefix 'CH' saved as part csv file inside CH folder'\n",
      "Merged CSV files for prefix 'DL' saved as part csv file inside DL folder'\n",
      "Merged CSV files for prefix 'GJ' saved as part csv file inside GJ folder'\n",
      "Merged CSV files for prefix 'HP' saved as part csv file inside HP folder'\n",
      "Merged CSV files for prefix 'HR' saved as part csv file inside HR folder'\n",
      "Merged CSV files for prefix 'JH' saved as part csv file inside JH folder'\n",
      "Merged CSV files for prefix 'JK' saved as part csv file inside JK folder'\n",
      "Merged CSV files for prefix 'KA' saved as part csv file inside KA folder'\n",
      "Merged CSV files for prefix 'KL' saved as part csv file inside KL folder'\n",
      "Merged CSV files for prefix 'MH' saved as part csv file inside MH folder'\n",
      "Merged CSV files for prefix 'ML' saved as part csv file inside ML folder'\n",
      "Merged CSV files for prefix 'MN' saved as part csv file inside MN folder'\n",
      "Merged CSV files for prefix 'MP' saved as part csv file inside MP folder'\n",
      "Merged CSV files for prefix 'MZ' saved as part csv file inside MZ folder'\n",
      "Merged CSV files for prefix 'NL' saved as part csv file inside NL folder'\n",
      "Merged CSV files for prefix 'OR' saved as part csv file inside OR folder'\n",
      "Merged CSV files for prefix 'PB' saved as part csv file inside PB folder'\n",
      "Merged CSV files for prefix 'PY' saved as part csv file inside PY folder'\n",
      "Merged CSV files for prefix 'RJ' saved as part csv file inside RJ folder'\n",
      "Merged CSV files for prefix 'SK' saved as part csv file inside SK folder'\n",
      "Merged CSV files for prefix 'TG' saved as part csv file inside TG folder'\n",
      "Merged CSV files for prefix 'TN' saved as part csv file inside TN folder'\n",
      "Merged CSV files for prefix 'TR' saved as part csv file inside TR folder'\n",
      "Merged CSV files for prefix 'UK' saved as part csv file inside UK folder'\n",
      "Merged CSV files for prefix 'UP' saved as part csv file inside UP folder'\n",
      "Merged CSV files for prefix 'WB' saved as part csv file inside WB folder'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVProcessor\").getOrCreate()\n",
    "\n",
    "# Define the directory paths\n",
    "directory_path = 'file:////home/talentum/myproject/datasource/archive'\n",
    "savepath = 'file:////home/talentum/myproject/datasource/output'\n",
    "\n",
    "# Define the prefixes you want to handle (assuming state_dict is defined elsewhere)\n",
    "prefixes = list(state_dict.values())  # Add other prefixes as needed\n",
    "\n",
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    normalized_columns = [col.strip().replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '') for col in df.columns]\n",
    "    return df.toDF(*normalized_columns)\n",
    "\n",
    "# Function to ensure correct data types\n",
    "def ensure_data_types(df, columns):\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            df = df.withColumn(column, lit(None).cast(StringType()))\n",
    "    return df.select(columns)\n",
    "\n",
    "# Process each prefix\n",
    "for prefix in prefixes:\n",
    "    # Define the search pattern\n",
    "    search_pattern = os.path.join(directory_path, f'{prefix}*.csv')\n",
    "    \n",
    "    # Read all CSV files using Spark's read method\n",
    "    df = spark.read.option(\"header\", \"true\").csv(search_pattern, inferSchema=True)\n",
    "    \n",
    "    if df.count() > 0:\n",
    "        # Normalize column names\n",
    "        df = normalize_column_names(df)\n",
    "        \n",
    "        # Identify all columns in the DataFrame\n",
    "        all_columns = sorted(df.columns)  # Sort columns for consistency\n",
    "        \n",
    "        # Ensure DataFrame has the correct columns and data types\n",
    "        df = ensure_data_types(df, all_columns)\n",
    "        \n",
    "        # Extract state abbreviation from file names (assuming prefix is the state abbreviation)\n",
    "        state_abbr = prefix\n",
    "        state_name = next((name for name, abbr in state_dict.items() if abbr == state_abbr), None)\n",
    "        \n",
    "        # Add the state column\n",
    "        df = df.withColumn('state', lit(state_name).cast(StringType()))\n",
    "        \n",
    "        # Save the DataFrame to a single CSV file\n",
    "        output_path = os.path.join(savepath, f'{prefix}')\n",
    "        df.coalesce(1).write.option(\"header\", \"true\").csv(output_path, mode='overwrite')\n",
    "        print(f\"Merged CSV files for prefix '{prefix}' saved as part csv file inside {prefix} folder'\")\n",
    "    else:\n",
    "        print(f\"No valid files found for prefix '{prefix}'.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb4e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------+-------------------+-------------+-------+---------+------+------------+-----------------+--------------+\n",
      "|at_degree_c|bp_mmhg|benzene_ug/m3|co_mg/m3|eth-benzene_ug/m3|          from_date|mp-xylene_ug/m3|nh3_ug/m3|no2_ug/m3|no_ug/m3|nox_ppb|ozone_ug/m3|pm10_ug/m3|pm2_5_ug/m3|rf_mm| rh_%|so2_ug/m3|sr_w/mt2|temp_degree_c|            to_date|toluene_ug/m3|vws_m/s|wd_degree|ws_m/s|xylene_ug/m3|             city|         state|\n",
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------+-------------------+-------------+-------+---------+------+------------+-----------------+--------------+\n",
      "|      28.18| 750.25|          0.1|     0.0|            28.57|2017-09-11 04:00:00|           0.03|     6.98|    11.25|     0.1|   6.03|       11.4|      6.25|       2.25|  0.0| 95.0|     5.77|     1.0|        29.65|2017-09-11 05:00:00|         2.27|   0.05|    82.75|  1.85|        0.18|Rajamahendravaram|Andhra Pradesh|\n",
      "|       31.6| 745.75|         0.23|    0.13|            28.57|2017-09-16 18:00:00|            0.1|     11.0|    14.77|    0.45|    8.2|      55.85|     37.75|      18.25|  0.0| 75.5|      6.1|    8.25|        31.23|2017-09-16 19:00:00|         2.85|    0.0|   255.75|   3.3|        0.63|Rajamahendravaram|Andhra Pradesh|\n",
      "|      28.03| 747.25|         0.25|    0.33|            28.57|2017-09-25 17:00:00|           0.18|     7.77|    18.35|    0.58|   10.2|      41.98|      31.0|       16.0| 0.05|90.25|     5.48|    0.05|        28.75|2017-09-25 18:00:00|         4.02|    0.0|   143.75|  1.72|        1.32|Rajamahendravaram|Andhra Pradesh|\n",
      "|      28.45| 750.25|         0.87|    1.25|            28.57|2017-10-05 21:00:00|            0.9|    10.08|     45.7|     5.4|  28.72|      18.72|     40.75|      20.75|  0.0|89.75|     19.9|     1.0|        29.27|2017-10-05 22:00:00|         8.05|    0.0|    134.0|  0.75|        4.55|Rajamahendravaram|Andhra Pradesh|\n",
      "|       27.5|  750.0|          0.2|     0.2|            28.57|2017-10-06 23:00:00|           0.13|     6.25|    13.37|    0.05|    7.0|      25.88|     12.75|        5.5|  0.0| 95.5|     5.88|     0.0|        29.55|2017-10-07 00:00:00|          2.5|    0.0|    95.75|   1.2|        0.88|Rajamahendravaram|Andhra Pradesh|\n",
      "|       30.5| 753.75|          0.6|    0.58|            28.57|2017-10-23 21:00:00|           0.38|    16.18|    27.85|    0.75|  15.38|      48.47|     96.75|       55.0|  0.0| 77.0|    13.88|     0.0|         30.9|2017-10-23 22:00:00|          4.7|   0.05|    86.75|  2.28|        2.33|Rajamahendravaram|Andhra Pradesh|\n",
      "|       28.7|  756.0|         0.63|    0.37|            28.57|2017-10-28 23:00:00|            0.1|     21.7|    16.53|    0.95|    9.3|      54.43|     116.0|       59.0|  0.0| 88.0|     7.32|     1.0|        31.05|2017-10-29 00:00:00|         2.57|    0.0|    104.0|  1.53|        0.83|Rajamahendravaram|Andhra Pradesh|\n",
      "|      27.78|  758.0|         0.73|    0.25|            28.57|2017-10-30 07:00:00|            0.1|    19.25|    28.92|    2.88|  17.75|      48.48|     118.5|      56.25|  0.0|69.25|     6.83|    0.05|         30.3|2017-10-30 08:00:00|          2.6|    0.0|    126.5|  1.68|        0.95|Rajamahendravaram|Andhra Pradesh|\n",
      "|       31.8|  753.0|         0.43|     0.3|            28.57|2017-11-08 17:00:00|           0.15|    12.03|    22.33|    1.07|  12.75|      23.55|     76.25|      25.75|  0.0| 55.0|    17.18|    0.05|        31.25|2017-11-08 18:00:00|         2.85|    0.0|   125.25|   5.2|        1.15|Rajamahendravaram|Andhra Pradesh|\n",
      "|      28.82|  759.0|         0.73|    0.57|            28.57|2017-12-02 10:00:00|           0.15|    18.38|     29.9|    1.68|  17.25|      23.55|     170.0|      84.75|  0.0| 55.0|      7.6|    0.05|        30.43|2017-12-02 11:00:00|         2.75|   0.05|     93.0|  3.17|        0.93|Rajamahendravaram|Andhra Pradesh|\n",
      "|      24.13|  755.0|          0.9|    1.34|            28.57|2017-12-06 22:00:00|           0.23|     24.1|    37.82|    1.28|  21.15|      63.55|      61.0|      27.25|  0.0|84.75|     4.28|     1.0|        28.65|2017-12-06 23:00:00|         3.35|  -0.03|   252.25|  1.33|        1.43|Rajamahendravaram|Andhra Pradesh|\n",
      "|      23.38|  761.0|         1.03|    1.25|            28.57|2017-12-20 20:00:00|           0.63|     21.5|    52.52|    1.05|  12.68|      84.48|    153.25|      86.25|  0.0| 70.5|    12.88|    0.05|        28.97|2017-12-20 21:00:00|         6.35|   0.03|     87.5|   1.8|        3.85|Rajamahendravaram|Andhra Pradesh|\n",
      "|      23.55|  744.0|         0.53|    0.57|            28.57|2018-01-02 21:00:00|           0.28|    10.03|    16.82|    2.88|  12.68|      23.55|      61.0|      27.25|  0.0| 86.0|      7.6|    0.05|        28.67|2018-01-02 22:00:00|         1.73|  -0.03|   215.25|   3.0|        1.92|Rajamahendravaram|Andhra Pradesh|\n",
      "|      21.95| 754.75|         0.35|    0.17|            28.57|2018-01-06 05:00:00|            0.1|    10.03|    18.45|    1.55|  12.68|       66.4|     93.75|      65.25|  0.0| 80.0|     2.17|    0.05|        28.15|2018-01-06 06:00:00|         0.78|    0.0|   171.25|  2.07|         0.1|Rajamahendravaram|Andhra Pradesh|\n",
      "|      29.43|  755.0|         0.47|    0.82|            28.57|2018-01-08 15:00:00|            0.1|    19.83|    22.13|    1.37|  12.68|      23.55|    120.33|       67.0|  0.0|46.67|     14.7|    0.05|        30.07|2018-01-08 16:00:00|          1.7|    0.0|   114.67|  4.83|         0.6|Rajamahendravaram|Andhra Pradesh|\n",
      "|      24.17|  756.0|         0.62|    1.19|            28.57|2018-01-23 07:00:00|           3.23|    10.03|    29.25|    2.75|  17.78|      50.57|    136.75|       75.5|  0.0| 85.5|    10.25|    0.05|        28.47|2018-01-23 08:00:00|         1.38|    0.0|   236.25|  1.68|        0.43|Rajamahendravaram|Andhra Pradesh|\n",
      "|      26.62| 753.75|         0.95|    1.46|            28.57|2018-01-23 19:00:00|           0.33|    23.53|     42.4|    2.15|   24.3|      60.95|     141.0|      75.25|  0.0| 71.0|    12.75|     6.0|         29.6|2018-01-23 20:00:00|         3.93|    0.0|   133.75|   3.0|        2.17|Rajamahendravaram|Andhra Pradesh|\n",
      "|       29.9| 756.33|         0.27|    0.79|            28.57|2018-01-28 12:00:00|           3.23|     24.9|    20.95|    0.85|  11.85|      23.55|    119.33|      61.33|  0.0| 54.0|    14.63|    0.05|        30.23|2018-01-28 13:00:00|         1.37|    0.0|   153.33|  2.97|        0.27|Rajamahendravaram|Andhra Pradesh|\n",
      "|      22.77|  756.0|          0.5|    1.12|            28.57|2018-01-31 03:00:00|           0.05|    10.03|    43.28|    2.92|  25.35|      36.57|    145.75|      88.75|  0.0| 93.5|    11.68|     6.0|         29.2|2018-01-31 04:00:00|         0.83|    0.0|   283.75|  1.67|        0.23|Rajamahendravaram|Andhra Pradesh|\n",
      "|       19.6|  760.5|         0.48|    1.49|            28.57|2018-02-06 07:00:00|           3.23|    10.03|    38.05|    4.25|   23.7|      22.28|    111.25|      53.25|  0.0|85.25|     11.4|    0.05|         28.5|2018-02-06 08:00:00|         1.28|    0.0|    199.0|  1.37|        0.43|Rajamahendravaram|Andhra Pradesh|\n",
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------+-------------------+-------------+-------+---------+------+------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Columns after cleaning: ['at_degree_c', 'bp_mmhg', 'benzene_ug/m3', 'co_mg/m3', 'eth-benzene_ug/m3', 'from_date', 'mp-xylene_ug/m3', 'nh3_ug/m3', 'no2_ug/m3', 'no_ug/m3', 'nox_ppb', 'ozone_ug/m3', 'pm10_ug/m3', 'pm2_5_ug/m3', 'rf_mm', 'rh_%', 'so2_ug/m3', 'sr_w/mt2', 'temp_degree_c', 'to_date', 'toluene_ug/m3', 'vws_m/s', 'wd_degree', 'ws_m/s', 'xylene_ug/m3', 'city', 'state']\n",
      "Cleaned data saved to /home/talentum/myproject/datasource/output/Cleaned/AP\n",
      "+-----------+-------+-------------+--------+-------------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+-------------------+---------+------+----------+-----------------+--------------------+\n",
      "|at_degree_c|bp_mmhg|benzene_ug/m3|co_mg/m3|          from_date|nh3_ug/m3|no2_ug/m3|no_ug/m3|nox_ppb|ozone_ug/m3|pm10_ug/m3|pm2_5_ug/m3|rf_mm| rh_%|so2_ug/m3|            to_date|wd_degree|ws_m/s|      city|            state|    station_location|\n",
      "+-----------+-------+-------------+--------+-------------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+-------------------+---------+------+----------+-----------------+--------------------+\n",
      "|      32.28| 986.73|          0.0|    0.57|2021-03-29 10:00:00|     1.18|     5.42|    2.17|   7.88|      20.77|      38.9|      27.81|  0.0|36.22|    19.19|2021-03-29 11:00:00|   129.85|  0.51|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-04-03 02:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-04-03 03:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-04-10 00:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-04-10 01:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      37.33| 984.27|          0.0|    0.26|2021-04-24 14:00:00|     0.86|      5.2|    1.44|   6.68|      31.29|     69.83|       17.3|  0.0|36.31|    20.14|2021-04-24 15:00:00|   132.33|  0.55|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-04-29 03:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-04-29 04:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      27.79| 988.37|          0.0|    0.31|2021-05-06 15:00:00|     1.82|     6.77|    2.02|   8.89|       13.1|     43.48|      20.83|  0.0|68.72|    22.77|2021-05-06 16:00:00|   142.03|  0.44|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      21.53| 986.36|          0.0|    0.27|2021-05-11 23:00:00|     0.13|     2.86|    1.86|   5.14|      14.26|      38.9|       6.69|  0.0|95.57|    21.35|2021-05-12 00:00:00|   256.85|   0.9|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-05-22 13:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-05-22 14:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-05-22 19:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-05-22 20:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-06-06 09:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-06-06 10:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      36.93| 980.54|          0.0|    0.22|2021-06-12 11:00:00|     0.47|     3.64|    1.59|   5.46|      12.65|     60.39|      10.65|  0.0|55.15|    25.05|2021-06-12 12:00:00|   131.52|  0.66|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-06-14 02:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-06-14 03:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-07-01 19:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-07-01 20:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      25.02| 987.12|          0.0|    0.29|2021-07-11 01:00:00|     1.28|     5.24|    1.53|   6.92|       7.68|      38.9|       17.0|  0.0|83.58|    23.26|2021-07-11 02:00:00|    169.4|   0.4|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|       25.7| 984.12|          0.0|    0.21|2021-07-15 22:00:00|     0.58|     2.99|    2.27|   5.81|       3.99|     20.07|       1.97|  0.0|93.41|    26.81|2021-07-15 23:00:00|   221.69|  0.29|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      30.45| 976.15|          0.0|    0.22|2021-07-27 08:00:00|     1.67|     4.53|     3.0|   8.19|       8.42|     23.61|        0.8|  0.0| 82.6|    27.45|2021-07-27 09:00:00|   139.69|  0.25|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|       26.6| 979.76|          0.0|    0.24|2021-08-11 01:00:00|      0.1|      2.5|     1.5|   4.31|       9.38|      16.5|       3.18|  0.0|95.61|    29.28|2021-08-11 02:00:00|   122.77|  0.21|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      26.99| 981.91|          0.0|    0.26|2021-08-12 05:00:00|     3.46|      7.5|    4.15|  12.48|       7.39|     25.33|       7.52|  0.0|95.52|    29.63|2021-08-12 06:00:00|   132.92|  0.22|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      24.14| 985.97|          0.0|    0.17|2021-09-08 04:00:00|     0.38|     1.74|    2.26|   4.64|       3.85|     19.85|       4.55|  0.0|97.36|    31.54|2021-09-08 05:00:00|   105.64|  0.19|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "|      28.27| 983.63|          0.0|    0.42|2021-09-13 20:00:00|     5.72|    10.28|    4.91|  16.02|       4.58|     67.25|      20.35|  0.0|89.22|    33.35|2021-09-13 21:00:00|   197.95|  0.31|Naharlagun|Arunachal Pradesh|Naharlagun, Nahar...|\n",
      "+-----------+-------+-------------+--------+-------------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+-------------------+---------+------+----------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Columns after cleaning: ['at_degree_c', 'bp_mmhg', 'benzene_ug/m3', 'co_mg/m3', 'from_date', 'nh3_ug/m3', 'no2_ug/m3', 'no_ug/m3', 'nox_ppb', 'ozone_ug/m3', 'pm10_ug/m3', 'pm2_5_ug/m3', 'rf_mm', 'rh_%', 'so2_ug/m3', 'to_date', 'wd_degree', 'ws_m/s', 'city', 'state', 'station_location']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to /home/talentum/myproject/datasource/output/Cleaned/AR\n",
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------------+---------+------+--------+-----+--------------------+\n",
      "|at_degree_c|bp_mmhg|benzene_ug/m3|co_mg/m3|eth-benzene_ug/m3|          from_date|mp-xylene_ug/m3|nh3_ug/m3|no2_ug/m3|no_ug/m3|nox_ppb|ozone_ug/m3|pm10_ug/m3|pm2_5_ug/m3|rf_mm| rh_%|so2_ug/m3|sr_w/mt2|            to_date|wd_degree|ws_m/s|    city|state|    station_location|\n",
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------------+---------+------+--------+-----+--------------------+\n",
      "|      24.18|1002.72|         1.24|    0.53|             0.45|2019-03-09 17:00:00|           0.67|    19.85|     9.11|    1.75|  12.65|       23.3|    101.69|      51.64|  0.0|38.57|    15.44|    4.71|2019-03-09 18:00:00|   288.55|  1.19|Guwahati|Assam|Railway Colony, G...|\n",
      "|       19.4|  988.2|         0.03|    0.49|             0.45|2019-03-10 21:00:00|           2.05|    19.77|      4.9|    2.09|   5.45|      11.56|    276.83|     176.96| 0.22|71.86|    24.55|    5.38|2019-03-10 22:00:00|   112.78|  0.92|Guwahati|Assam|Railway Colony, G...|\n",
      "|      27.91|1001.77|         1.21|    0.71|             0.45|2019-03-18 15:00:00|           0.52|    23.61|    11.22|    2.01|  15.27|      33.83|    157.73|      81.53|  0.0|37.56|    18.31|   90.62|2019-03-18 16:00:00|   252.42|  1.24|Guwahati|Assam|Railway Colony, G...|\n",
      "|      29.85| 995.58|          0.1|    0.34|             0.45|2019-05-05 16:00:00|           0.42|    25.89|     6.08|    1.35|   8.68|       18.0|     32.34|      20.36|  0.0|69.06|    24.85|  163.93|2019-05-05 17:00:00|   276.93|  1.48|Guwahati|Assam|Railway Colony, G...|\n",
      "|      25.09| 999.78|          0.3|    0.35|             0.04|2019-05-07 10:00:00|           0.04|    13.56|      7.2|    1.31|   9.87|      16.65|     36.06|      17.92|  0.0|77.18|    23.79|  587.83|2019-05-07 11:00:00|    83.33|  2.71|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.18| 997.51|         0.23|    0.38|             0.45|2019-05-08 10:00:00|           0.03|     9.36|     8.49|    2.12|   12.6|      16.65|     41.19|      29.54|  0.0|71.94|    18.41|   62.32|2019-05-08 11:00:00|    86.43|  2.38|Guwahati|Assam|Railway Colony, G...|\n",
      "|      22.95| 996.32|         0.04|     0.3|             0.45|2019-05-18 01:00:00|           0.42|     6.58|     7.96|    2.95|  13.31|      16.65|     22.44|      16.17|  0.0| 93.0|    16.44|   62.32|2019-05-18 02:00:00|    103.3|   1.9|Guwahati|Assam|Railway Colony, G...|\n",
      "|      27.96| 997.91|         0.09|    0.36|             0.45|2019-05-18 12:00:00|           0.42|      8.0|     7.41|    1.94|  11.02|      16.65|     26.16|      18.68|  0.0|75.14|    16.18|   62.32|2019-05-18 13:00:00|   125.26|  1.52|Guwahati|Assam|Railway Colony, G...|\n",
      "|      24.01| 975.81|         0.26|    0.35|             0.45|2019-05-24 15:00:00|            0.4|    12.15|    12.19|    1.89|  16.17|      16.65|     12.56|       8.94|  0.0|84.08|    16.13|  105.94|2019-05-24 16:00:00|    73.08|  1.22|Guwahati|Assam|Railway Colony, G...|\n",
      "|       27.3| 980.71|         0.09|    0.34|             0.16|2019-06-08 07:00:00|           0.49|    10.69|     7.61|    4.45|  15.56|      25.68|     20.38|      15.73|  0.0|80.69|     16.8|  195.32|2019-06-08 08:00:00|    263.3|  0.93|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.15| 995.59|         0.27|    0.45|             0.22|2019-06-08 22:00:00|           0.54|     9.46|     6.97|    3.46|  13.09|      25.25|     23.58|       10.0|  0.0|88.77|    16.54|   62.32|2019-06-08 23:00:00|   124.88|  0.43|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.45| 989.62|          0.0|    0.17|             0.45|2019-07-06 04:00:00|           0.42|     4.88|     4.25|    2.01|   7.82|      17.33|       7.0|       2.79|  0.0|91.02|    10.46|    1.14|2019-07-06 05:00:00|    73.97|  1.37|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.77| 995.84|         0.28|    0.85|             0.45|2019-07-12 21:00:00|           0.42|     9.57|    10.08|    4.15|  17.65|       9.28|     65.91|      48.45|  0.0|90.53|    10.31|   62.32|2019-07-12 22:00:00|   250.08|  1.17|Guwahati|Assam|Railway Colony, G...|\n",
      "|       29.5| 995.69|         0.19|    0.33|             0.03|2019-07-17 09:00:00|           0.24|     6.99|     6.87|    1.68|  10.15|       8.47|     33.88|      21.42|  0.0|73.66|    10.77|   62.32|2019-07-17 10:00:00|   277.35|  1.83|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.03|  994.0|         0.01|    0.35|             0.07|2019-07-18 02:00:00|           0.41|     7.23|     7.81|    2.41|  12.32|       3.42|     38.69|      28.77|  0.0|91.02|     10.9|   62.32|2019-07-18 03:00:00|   108.77|  0.33|Guwahati|Assam|Railway Colony, G...|\n",
      "|      27.99| 995.25|          0.0|    0.57|             0.43|2019-07-24 20:00:00|           1.36|     8.14|    27.97|   11.66|   5.45|      15.23|     64.58|      37.29|  0.0|83.93|     10.5|   62.32|2019-07-24 21:00:00|   134.51|  0.43|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.47| 994.24|          0.0|    0.75|             0.35|2019-07-25 04:00:00|            1.0|     8.63|    12.17|    2.09|   5.45|      14.64|     70.41|      37.94|  0.0|91.88|    10.77|    0.83|2019-07-25 05:00:00|   139.82|  0.45|Guwahati|Assam|Railway Colony, G...|\n",
      "|      26.37| 993.71|         0.03|    0.35|             0.45|2019-08-13 03:00:00|           0.42|     4.95|     8.01|    5.95|  18.39|      15.28|     72.92|      14.38|  0.0|91.33|    11.23|   62.32|2019-08-13 04:00:00|    116.8|   1.2|Guwahati|Assam|Railway Colony, G...|\n",
      "|      23.92| 991.62|         0.03|    0.49|             0.45|2019-08-31 06:00:00|           0.42|     6.54|      4.9|    2.09|   5.45|      16.65|     72.92|       39.6| null|72.64|    14.42|   62.32|2019-08-31 07:00:00|    119.8|  0.89|Guwahati|Assam|Railway Colony, G...|\n",
      "|       26.4| 994.02|          0.0|    1.06|             0.45|2019-09-10 00:00:00|           0.42|     2.79|     7.73|    2.09|   5.45|      15.51|     65.33|      25.14|  0.0| 90.8|    12.49|   62.32|2019-09-10 01:00:00|   153.48|  0.36|Guwahati|Assam|Railway Colony, G...|\n",
      "+-----------+-------+-------------+--------+-----------------+-------------------+---------------+---------+---------+--------+-------+-----------+----------+-----------+-----+-----+---------+--------+-------------------+---------+------+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Columns after cleaning: ['at_degree_c', 'bp_mmhg', 'benzene_ug/m3', 'co_mg/m3', 'eth-benzene_ug/m3', 'from_date', 'mp-xylene_ug/m3', 'nh3_ug/m3', 'no2_ug/m3', 'no_ug/m3', 'nox_ppb', 'ozone_ug/m3', 'pm10_ug/m3', 'pm2_5_ug/m3', 'rf_mm', 'rh_%', 'so2_ug/m3', 'sr_w/mt2', 'to_date', 'wd_degree', 'ws_m/s', 'city', 'state', 'station_location']\n",
      "Cleaned data saved to /home/talentum/myproject/datasource/output/Cleaned/AS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e05b86837847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Process all files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mprocess_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Stop the Spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e05b86837847>\u001b[0m in \u001b[0;36mprocess_all_files\u001b[0;34m(state_codes, base_path, output_dir)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mprocess_csv_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Define state codes, base path, and output directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e05b86837847>\u001b[0m in \u001b[0;36mprocess_csv_file\u001b[0;34m(file_path, output_dir, state_code)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mnumeric_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Remove duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e05b86837847>\u001b[0m in \u001b[0;36mhandle_outliers\u001b[0;34m(df, column)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhandle_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"\"\"Handle outliers in a specific column using the IQR method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mQ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mIQR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, mean as _mean, to_date, count, isnull\n",
    "from pyspark.sql.types import DoubleType\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "def handle_outliers(df, column):\n",
    "    \"\"\"Handle outliers in a specific column using the IQR method.\"\"\"\n",
    "    Q1 = df.approxQuantile(column, [0.25], 0.01)[0]\n",
    "    Q3 = df.approxQuantile(column, [0.75], 0.01)[0]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df.withColumn(column, when((col(column) < lower_bound) | (col(column) > upper_bound), None).otherwise(col(column)))\n",
    "    median_value = df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "    df = df.fillna({column: median_value})\n",
    "    return df\n",
    "\n",
    "def process_csv_file(file_path, output_dir, state_code):\n",
    "    \"\"\"Read, clean, and save a single CSV file.\"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path, inferSchema=True)\n",
    "    \n",
    "    total_columns = len(df.columns)\n",
    "    missing_threshold_row = total_columns * 0.85\n",
    "    df = df.withColumn('missing_count', sum([isnull(col(c)).cast('int') for c in df.columns]))\n",
    "    df = df.filter(col('missing_count') <= missing_threshold_row).drop('missing_count')\n",
    "\n",
    "    # Step 2: Drop columns with more than 50% missing values (based on the new row count)\n",
    "    total_rows = df.count()\n",
    "    missing_threshold_col = total_rows * 0.60\n",
    "    missing_value_counts = {c: df.filter(isnull(col(c))).count() for c in df.columns}\n",
    "    cols_to_drop = [c for c in missing_value_counts if missing_value_counts[c] > missing_threshold_col]\n",
    "    df = df.drop(*cols_to_drop)\n",
    "    \n",
    "    # Handle outliers and impute missing values for numeric columns\n",
    "    numeric_columns = [f.name for f in df.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "    for column in numeric_columns:\n",
    "        df = handle_outliers(df, column)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    # Convert date columns\n",
    "    if 'From Date' in df.columns and 'To Date' in df.columns:\n",
    "        df = df.withColumn('From_Date', to_date(col('From Date'), 'yyyy-MM-dd'))\n",
    "        df = df.withColumn('To_Date', to_date(col('To Date'), 'yyyy-MM-dd'))\n",
    "        df = df.drop('From Date', 'To Date')\n",
    "    \n",
    "    # Normalize column names\n",
    "    normalized_columns = [col.strip().replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '').lower() for col in df.columns]\n",
    "    df = df.toDF(*normalized_columns)\n",
    "    \n",
    "    # Show the DataFrame and print remaining columns\n",
    "    df.show()\n",
    "    print(f\"Columns after cleaning: {df.columns}\")\n",
    "    \n",
    "    # Print a red warning if the number of columns is less than 12\n",
    "    if len(df.columns) < 12:\n",
    "        print(\"\\033[91mWarning: The number of columns after cleaning is less than 10.\\033[0m\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save the cleaned DataFrame to a single CSV file\n",
    "    output_file = os.path.join(output_dir, f'{state_code}')\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(f'file://{output_file}', mode='overwrite')\n",
    "    print(f'Cleaned data saved to {output_file}')\n",
    "\n",
    "def process_all_files(state_codes, base_path, output_dir):\n",
    "    \"\"\"Process CSV files from multiple state codes.\"\"\"\n",
    "    for state_code in state_codes:\n",
    "        file_path = os.path.join(base_path, state_code, '*.csv')\n",
    "        process_csv_file(file_path, output_dir, state_code)\n",
    "\n",
    "# Define state codes, base path, and output directory\n",
    "state_codes = list(state_dict.values())  # Example state codes\n",
    "# state_codes = ['JH']\n",
    "base_path = 'file:///home/talentum/myproject/datasource/output'\n",
    "output_dir = '/home/talentum/myproject/datasource/output/Cleaned'\n",
    "\n",
    "# Process all files\n",
    "process_all_files(state_codes, base_path, output_dir)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "# base_cleaned_file_path='hdfs:///user/talentum/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221d5776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/AP/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/AR/*.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"temp_degree_c\" among (at_degree_c, bp_mmhg, benzene_ug/m3, co_mg/m3, eth-benzene_ug/m3, from_date, mp-xylene_ug/m3, nh3_ug/m3, no2_ug/m3, no_ug/m3, nox_ppb, ozone_ug/m3, pm10_ug/m3, pm2_5_ug/m3, rf_mm, rh_%, so2_ug/m3, sr_w/mt2, to_date, wd_degree, ws_m/s, state);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o11103.unionByName.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"temp_degree_c\" among (at_degree_c, bp_mmhg, benzene_ug/m3, co_mg/m3, eth-benzene_ug/m3, from_date, mp-xylene_ug/m3, nh3_ug/m3, no2_ug/m3, no_ug/m3, nox_ppb, ozone_ug/m3, pm10_ug/m3, pm2_5_ug/m3, rf_mm, rh_%, so2_ug/m3, sr_w/mt2, to_date, wd_degree, ws_m/s, state);\n\tat org.apache.spark.sql.Dataset$$anonfun$23$$anonfun$apply$10.apply(Dataset.scala:1912)\n\tat org.apache.spark.sql.Dataset$$anonfun$23$$anonfun$apply$10.apply(Dataset.scala:1912)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset$$anonfun$23.apply(Dataset.scala:1911)\n\tat org.apache.spark.sql.Dataset$$anonfun$23.apply(Dataset.scala:1910)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.Dataset.unionByName(Dataset.scala:1910)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb2b29f17996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished merging all CSV files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munionByName\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \"\"\"\n\u001b[0;32m-> 1499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"temp_degree_c\" among (at_degree_c, bp_mmhg, benzene_ug/m3, co_mg/m3, eth-benzene_ug/m3, from_date, mp-xylene_ug/m3, nh3_ug/m3, no2_ug/m3, no_ug/m3, nox_ppb, ozone_ug/m3, pm10_ug/m3, pm2_5_ug/m3, rf_mm, rh_%, so2_ug/m3, sr_w/mt2, to_date, wd_degree, ws_m/s, state);'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"Merge CSV to Parquet\").getOrCreate()\n",
    "\n",
    "directories = [\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AS/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/BR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/CG/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/CH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/DL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/GJ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/HP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/HR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/JH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/JK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/KA/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/KL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/ML/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MN/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MZ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/NL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/OR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/PB/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/PY/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/RJ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/SK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TG/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TN/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/UK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/UP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/WB/*.csv\"\n",
    "]\n",
    "# Define the list of directories\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "merged_df = None\n",
    "\n",
    "# Loop through the directories and read CSV files\n",
    "for directory in directories:\n",
    "    print(f\"Reading files from: {directory}\")\n",
    "    df = spark.read.csv(directory, header=True, inferSchema=True)\n",
    "    \n",
    "    # If merged_df is None, set it to the current df, otherwise union it with the existing merged_df\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = merged_df.unionByName(df)\n",
    "\n",
    "print(\"Finished merging all CSV files.\")\n",
    "\n",
    "# Save the merged DataFrame as a single Parquet file\n",
    "output_path = \"file:///home/talentum/myproject/dataSource/output/india_region.parquet\"\n",
    "merged_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Finished saving the merged DataFrame as Parquet at {output_path}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af44b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at_degree_c', 'bp_mmhg', 'benzene_ug/m3', 'co_mg/m3', 'eth-benzene_ug/m3', 'from_date', 'mp-xylene_ug/m3', 'nh3_ug/m3', 'no2_ug/m3', 'no_ug/m3', 'nox_ppb', 'ozone_ug/m3', 'pm10_ug/m3', 'pm2_5_ug/m3', 'rf_mm', 'rh_%', 'so2_ug/m3', 'sr_w/mt2', 'to_date', 'wd_degree', 'ws_m/s', 'state']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b483f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to read CSV files...\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/AP/*.csv\n",
      "Read 225716 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/AP/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/AR/*.csv\n",
      "Read 8131 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/AR/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/AS/*.csv\n",
      "Read 73172 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/AS/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/BR/*.csv\n",
      "Read 636501 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/BR/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/CG/*.csv\n",
      "Read 49048 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/CG/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/CH/*.csv\n",
      "Read 54786 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/CH/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/DL/*.csv\n",
      "Read 2118037 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/DL/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/GJ/*.csv\n",
      "Read 346017 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/GJ/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/HP/*.csv\n",
      "Read 8851 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/HP/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/HR/*.csv\n",
      "Read 1110835 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/HR/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/JH/*.csv\n",
      "Read 52624 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/JH/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/JK/*.csv\n",
      "Read 16792 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/JK/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/KA/*.csv\n",
      "Read 1087206 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/KA/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/KL/*.csv\n",
      "Read 264335 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/KL/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/MH/*.csv\n",
      "Read 1264555 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/MH/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/ML/*.csv\n",
      "Read 30422 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/ML/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/MN/*.csv\n",
      "Read 14918 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/MN/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/MP/*.csv\n",
      "Read 572106 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/MP/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/MZ/*.csv\n",
      "Read 24991 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/MZ/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/NL/*.csv\n",
      "Read 21414 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/NL/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/OR/*.csv\n",
      "Read 102796 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/OR/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/PB/*.csv\n",
      "Read 374848 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/PB/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/PY/*.csv\n",
      "Read 19109 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/PY/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/RJ/*.csv\n",
      "Read 485589 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/RJ/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/SK/*.csv\n",
      "Read 8048 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/SK/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/TG/*.csv\n",
      "Read 390250 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/TG/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/TN/*.csv\n",
      "Read 518525 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/TN/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/TR/*.csv\n",
      "Read 29013 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/TR/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/UK/*.csv\n",
      "Read 10560 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/UK/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/UP/*.csv\n",
      "Read 1641091 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/UP/*.csv\n",
      "Reading files from: file:///home/talentum/myproject/datasource/output/Cleaned/WB/*.csv\n",
      "Read 559152 rows from: file:///home/talentum/myproject/datasource/output/Cleaned/WB/*.csv\n",
      "Finished reading CSV files.\n",
      "Starting to clean column names...\n",
      "Finished cleaning column names.\n",
      "Starting to ensure all DataFrames have the same columns...\n",
      "Finished ensuring all DataFrames have the same columns.\n",
      "Starting to select columns in the same order for consistency...\n",
      "Finished selecting columns in the same order.\n",
      "Starting to merge DataFrames...\n",
      "Finished merging DataFrames.\n",
      "Starting to save the merged DataFrame to a Parquet file...\n",
      "Finished saving the merged DataFrame as Parquet.\n",
      "Stopping the Spark session...\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Merge\").getOrCreate()\n",
    "\n",
    "# List of directories (replace with your actual list)\n",
    "directories = [\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/AS/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/BR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/CG/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/CH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/DL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/GJ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/HP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/HR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/JH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/JK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/KA/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/KL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MH/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/ML/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MN/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/MZ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/NL/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/OR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/PB/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/PY/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/RJ/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/SK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TG/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TN/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/TR/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/UK/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/UP/*.csv\",\n",
    "    \"file:///home/talentum/myproject/datasource/output/Cleaned/WB/*.csv\"\n",
    "]\n",
    "\n",
    "\n",
    "dfs = []\n",
    "all_columns = set()\n",
    "\n",
    "print(\"Starting to read CSV files...\")\n",
    "\n",
    "# Read the CSV files and update the list of all columns\n",
    "for directory in directories:\n",
    "    print(f\"Reading files from: {directory}\")\n",
    "    df = spark.read.csv(directory, header=True, inferSchema=True)\n",
    "    print(f\"Read {df.count()} rows from: {directory}\")\n",
    "    dfs.append(df)\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "print(\"Finished reading CSV files.\")\n",
    "print(\"Starting to clean column names...\")\n",
    "\n",
    "# Clean column names\n",
    "def clean_column_names(df):\n",
    "    for col_name in df.columns:\n",
    "        new_col_name = col_name.replace('-', '_').replace('/', '_').replace(' ', '_')\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "    return df\n",
    "\n",
    "dfs = [clean_column_names(df) for df in dfs]\n",
    "all_columns = {col_name.replace('-', '_').replace('/', '_').replace(' ', '_') for col_name in all_columns}\n",
    "\n",
    "print(\"Finished cleaning column names.\")\n",
    "print(\"Starting to ensure all DataFrames have the same columns...\")\n",
    "\n",
    "# Ensure all DataFrames have all columns\n",
    "def add_missing_columns(df, all_columns):\n",
    "    for column in all_columns:\n",
    "        if column not in df.columns:\n",
    "            df = df.withColumn(column, lit(None))\n",
    "    return df\n",
    "\n",
    "dfs = [add_missing_columns(df, all_columns) for df in dfs]\n",
    "\n",
    "print(\"Finished ensuring all DataFrames have the same columns.\")\n",
    "print(\"Starting to select columns in the same order for consistency...\")\n",
    "\n",
    "# Select the columns in the same order for consistency\n",
    "dfs = [df.select(*sorted(all_columns)) for df in dfs]\n",
    "\n",
    "print(\"Finished selecting columns in the same order.\")\n",
    "print(\"Starting to merge DataFrames...\")\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.unionByName(df)\n",
    "\n",
    "print(\"Finished merging DataFrames.\")\n",
    "print(\"Starting to save the merged DataFrame to a Parquet file...\")\n",
    "\n",
    "# Coalesce to a single partition and save the merged DataFrame to a Parquet file\n",
    "merged_df.coalesce(1).write.parquet(\"file:///home/talentum/myproject/datasource/output/merged_final\")\n",
    "\n",
    "print(\"Finished saving the merged DataFrame as Parquet.\")\n",
    "print(\"Stopping the Spark session...\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006ca0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"Parquet to CSV\").getOrCreate()\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/merged_final\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Define the output directory for the CSV file\n",
    "output_csv_path = \"file:///home/talentum/myproject/dataSource/output/merged_final_csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.coalesce(1).write.csv(output_csv_path, header=True)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d206748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the first 20 rows:\n",
      "+---------+-----------+-------------+-------+--------+-----------------+----------------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+-----+-----+---------+---------+--------+--------------+-----+-------------+----------------+-------------+-------+------+---------+------+------------+\n",
      "|at_degree|at_degree_c|benzene_ug_m3|bp_mmhg|co_mg_m3|eth_benzene_ug_m3|       from_date|mp_xylene_ug_m3|nh3_ug_m3|no2_ug_m3|no_ug_m3|nox_ppb|nox_ug_m3|o_xylene_ug_m3|ozone_ug_m3|pm10_ug_m3|pm2_5_ug_m3|rf_mm| rh_%|rh_degree|so2_ug_m3|sr_w_mt2|         state|temp_|temp_degree_c|         to_date|toluene_ug_m3|vws_m_s|wd_deg|wd_degree|ws_m_s|xylene_ug_m3|\n",
      "+---------+-----------+-------------+-------+--------+-----------------+----------------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+-----+-----+---------+---------+--------+--------------+-----+-------------+----------------+-------------+-------+------+---------+------+------------+\n",
      "|     null|      21.47|          0.4|  218.5|    0.26|             null|06-07-2016 22:00|           null|     6.88|    25.78|     3.4|  15.08|     null|          null|      13.78|      12.5|       3.75|  0.0|78.75|     null|     3.28|    10.0|Andhra Pradesh| null|         32.6|06-07-2016 23:00|          3.7|   -0.1| 298.0|     null|  2.07|         0.1|\n",
      "|     null|      27.97|         1.13|  218.5|    0.46|             null|16-07-2016 12:00|           null|    10.02|    32.47|    2.92|  12.78|     null|          null|       15.5|      47.0|      12.33|  0.0|59.33|     null|     5.33|     3.0|Andhra Pradesh| null|        33.93|16-07-2016 13:00|          7.8|    0.2| 328.0|     null|  1.03|         0.5|\n",
      "|     null|      22.37|          0.7|  218.5|    0.55|             null|29-07-2016 17:00|           null|     8.47|    51.73|    2.92|   44.8|     null|          null|      11.87|     24.33|        4.0|  0.0|74.33|     null|      6.2|     3.0|Andhra Pradesh| null|        32.93|29-07-2016 18:00|         8.07|   0.27|  27.0|     null|  1.23|         0.1|\n",
      "|     null|      26.47|         0.85|  218.5|    0.57|             null|12-08-2016 15:00|           null|      7.7|     61.2|    2.92|  49.55|     null|          null|       9.15|     89.25|      18.75|  0.0| 59.5|     null|      5.0|     3.0|Andhra Pradesh| null|        33.45|12-08-2016 16:00|          7.2|    1.0| 335.0|     null|  1.12|         0.1|\n",
      "|     null|       24.0|          1.0|  218.5|    0.27|             null|13-08-2016 23:00|           null|     6.73|    55.72|   20.03|  42.05|     null|          null|      17.85|      75.5|      29.25|  0.0|65.25|     null|     7.25|     6.0|Andhra Pradesh| null|        33.02|14-08-2016 00:00|          7.4|   0.23| 353.0|     null|  1.33|         0.1|\n",
      "|     null|      23.37|         0.57|  218.5|    0.28|             null|30-08-2016 00:00|           null|     13.3|     35.8|    7.17|  22.77|     null|          null|       14.1|      24.0|        7.0|  0.0| 70.0|     null|     4.57|     7.0|Andhra Pradesh| null|        33.97|30-08-2016 01:00|          7.8|   -0.1|345.67|     null|  1.17|         0.1|\n",
      "|     null|       22.0|          0.4|  218.5|    0.41|             null|09-09-2016 05:00|           null|     7.45|    60.85|   10.38|   37.3|     null|          null|      13.45|     49.75|      21.25|  0.0| 68.0|     null|      6.3|     6.0|Andhra Pradesh| null|        33.38|09-09-2016 06:00|          7.2|   1.12| 342.0|     null|  1.18|         0.1|\n",
      "|     null|      22.27|         0.77|  218.5|    0.43|             null|10-09-2016 23:00|           null|     9.45|    60.18|    9.87|  36.68|     null|          null|      16.55|     39.75|      12.25|  0.0|71.75|     null|     4.45|     6.0|Andhra Pradesh| null|        33.15|11-09-2016 00:00|          7.6|   0.55| 342.5|     null|  1.47|         0.1|\n",
      "|     null|      23.05|          1.0|  218.5|    0.63|             null|12-09-2016 13:00|           null|     7.45|    55.07|   19.42|   41.4|     null|          null|      16.65|     46.75|       19.5|  0.0| 70.0|     null|     5.58|     3.0|Andhra Pradesh| null|         33.6|12-09-2016 14:00|          7.5|   -0.1|345.25|     null|   1.7|         0.6|\n",
      "|     null|       21.6|          0.3|  218.5|    0.28|             null|22-09-2016 23:00|           null|      6.4|     33.1|    2.98|  18.35|     null|          null|      16.77|     22.75|       2.25|  0.0|74.75|     null|     4.88|     6.0|Andhra Pradesh| null|        33.18|23-09-2016 00:00|          3.7|   -0.1| 96.75|     null|  2.67|         0.1|\n",
      "|     null|       22.9|          1.4|  218.5|     0.5|             null|05-10-2016 20:00|           null|     12.0|    17.05|    2.92|  12.78|     null|          null|      18.33|     52.33|      21.67|  0.0| 66.0|     null|      6.7|     6.0|Andhra Pradesh| null|        33.67|05-10-2016 21:00|        10.57|    0.2| 190.0|     null|  0.37|         0.9|\n",
      "|     null|      27.67|          1.5|  218.5|    0.45|             null|23-10-2016 13:00|           null|     12.1|    17.05|     4.6|  48.27|     null|          null|       12.6|      78.0|      46.33|  0.0| 44.0|     null|      8.4|     3.0|Andhra Pradesh| null|        33.13|23-10-2016 14:00|          6.6|    0.2| 187.0|     null|   1.4|         0.5|\n",
      "|     null|      19.97|          1.3|  218.5|    0.62|             null|04-11-2016 06:00|           null|      9.6|    17.05|    2.92|  12.78|     null|          null|      18.12|      97.5|       59.5|  0.0|81.75|     null|      5.6|     3.0|Andhra Pradesh| null|        33.32|04-11-2016 07:00|         5.77|   -0.1| 290.0|     null|  2.22|        3.55|\n",
      "|     null|       25.7|          1.6|  218.5|    0.44|             null|06-11-2016 08:00|           null|    12.22|    17.05|    7.88|  12.78|     null|          null|       16.2|     92.75|       45.0|  0.0| 52.0|     null|     7.98|     3.0|Andhra Pradesh| null|        32.57|06-11-2016 09:00|          6.0|   0.23| 177.5|     null|  0.53|         2.4|\n",
      "|     null|      23.63|          1.6|  218.5|    0.62|             null|12-11-2016 15:00|           null|     10.3|    17.05|    7.13|  12.78|     null|          null|      17.73|    106.33|       71.0|  0.0|56.33|     null|    12.27|     3.0|Andhra Pradesh| null|         33.4|12-11-2016 16:00|         4.23|   -0.1|228.33|     null|  2.33|        1.07|\n",
      "|     null|      16.53|          1.3|  218.5|    0.46|             null|25-11-2016 01:00|           null|    10.02|    17.05|    5.48|   60.1|     null|          null|      17.55|    101.25|      71.25|  0.0|80.25|     null|     6.52|     7.0|Andhra Pradesh| null|         32.5|25-11-2016 02:00|          3.8|   -0.1|226.25|     null|  1.75|         0.1|\n",
      "|     null|      16.73|          2.1|  218.5|    0.71|             null|26-11-2016 00:00|           null|     10.9|    17.05|    2.92|  12.78|     null|          null|       17.4|     135.0|       77.0|  0.0|83.67|     null|     1.93|     7.0|Andhra Pradesh| null|         32.2|26-11-2016 01:00|          4.9|   -0.1| 279.0|     null|   1.8|         0.9|\n",
      "|     null|      19.82|         1.75|  218.5|    0.81|             null|29-11-2016 18:00|           null|    10.35|    17.05|    2.92|  12.78|     null|          null|       17.9|    113.75|      71.75|  0.0|70.75|     null|    10.07|     6.0|Andhra Pradesh| null|        32.83|29-11-2016 19:00|          5.7|   -0.1| 167.0|     null|  0.98|        1.05|\n",
      "|     null|      18.67|          0.5|  218.5|    0.42|             null|05-12-2016 03:00|           null|     7.43|    17.05|    2.92|   57.9|     null|          null|      10.73|     23.67|        5.0|  0.0| 86.0|     null|      3.4|    13.0|Andhra Pradesh| null|        32.63|05-12-2016 04:00|          4.0|   -0.1|  91.0|     null|   1.9|         0.1|\n",
      "|     null|      19.47|          3.1|  218.5|    0.78|             null|05-12-2016 19:00|           null|     9.77|    17.05|    17.9|  58.43|     null|          null|       17.8|    102.67|       67.0|  0.0| 86.0|     null|      5.4|     7.0|Andhra Pradesh| null|         32.5|05-12-2016 20:00|          6.0|   -0.1| 87.33|     null|  2.43|        1.07|\n",
      "+---------+-----------+-------------+-------+--------+-----------------+----------------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+-----+-----+---------+---------+--------+--------------+-----+-------------+----------------+-------------+-------+------+---------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Columns in the DataFrame:\n",
      "['at_degree', 'at_degree_c', 'benzene_ug_m3', 'bp_mmhg', 'co_mg_m3', 'eth_benzene_ug_m3', 'from_date', 'mp_xylene_ug_m3', 'nh3_ug_m3', 'no2_ug_m3', 'no_ug_m3', 'nox_ppb', 'nox_ug_m3', 'o_xylene_ug_m3', 'ozone_ug_m3', 'pm10_ug_m3', 'pm2_5_ug_m3', 'rf_mm', 'rh_%', 'rh_degree', 'so2_ug_m3', 'sr_w_mt2', 'state', 'temp_', 'temp_degree_c', 'to_date', 'toluene_ug_m3', 'vws_m_s', 'wd_deg', 'wd_degree', 'ws_m_s', 'xylene_ug_m3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 12119438\n",
      "Number of null values in each column:\n",
      "+---------+-----------+-------------+-------+--------+-----------------+---------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+--------+------+---------+---------+--------+-----+--------+-------------+-------+-------------+-------+-------+---------+------+------------+\n",
      "|at_degree|at_degree_c|benzene_ug_m3|bp_mmhg|co_mg_m3|eth_benzene_ug_m3|from_date|mp_xylene_ug_m3|nh3_ug_m3|no2_ug_m3|no_ug_m3|nox_ppb|nox_ug_m3|o_xylene_ug_m3|ozone_ug_m3|pm10_ug_m3|pm2_5_ug_m3|   rf_mm|  rh_%|rh_degree|so2_ug_m3|sr_w_mt2|state|   temp_|temp_degree_c|to_date|toluene_ug_m3|vws_m_s| wd_deg|wd_degree|ws_m_s|xylene_ug_m3|\n",
      "+---------+-----------+-------------+-------+--------+-----------------+---------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+--------+------+---------+---------+--------+-----+--------+-------------+-------+-------------+-------+-------+---------+------+------------+\n",
      "| 12108878|    7084985|       393359|2268611|   52624|          8198376|        0|        5358877|  3807766|        0|       0| 155420| 12016642|       5700987|      49048|   2137586|      52624|10184848|155420| 12016642|        0| 1861137|    0|10923785|      3618911|      0|       556254|6820905|1083827| 11088235| 52624|     8980485|\n",
      "+---------+-----------+-------------+-------+--------+-----------------+---------+---------------+---------+---------+--------+-------+---------+--------------+-----------+----------+-----------+--------+------+---------+---------+--------+-----+--------+-------------+-------+-------------+-------+-------+---------+------+------------+\n",
      "\n",
      "Mean of each numeric column:\n",
      "+------------------+------------------+-----------------+------------------+----------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+---------------+------------------+------------------+------------------+-----------------+\n",
      "|         at_degree|       at_degree_c|    benzene_ug_m3|           bp_mmhg|        co_mg_m3| eth_benzene_ug_m3|   mp_xylene_ug_m3|         nh3_ug_m3|         no2_ug_m3|          no_ug_m3|          nox_ppb|        nox_ug_m3|    o_xylene_ug_m3|       ozone_ug_m3|        pm10_ug_m3|      pm2_5_ug_m3|            rf_mm|             rh_%|        rh_degree|        so2_ug_m3|         sr_w_mt2|             temp_|    temp_degree_c|    toluene_ug_m3|        vws_m_s|            wd_deg|         wd_degree|            ws_m_s|     xylene_ug_m3|\n",
      "+------------------+------------------+-----------------+------------------+----------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+---------------+------------------+------------------+------------------+-----------------+\n",
      "|20.882438446968496|125.91642239178425|3.390727603824288|240.16749676541804|4.65922294899536|6.8677338154794505|20.937019207435732|20.158700677822946|20.378847972157768|26.156524423816908|24.76468210342856|23.16814389664788|17.587370607029737|16.920170702023483|114.64107904635502|55.20185924884177|83.62390103339196|30.69069131539577|68.30914091992285|11.48797650436581|84.33422298389797|24.486287451278656|45.76942340515009|9.889595738519464|86.934954042931|100.45364195812007|130.35903699844923|160.31151365913186|60.82195693283686|\n",
      "+------------------+------------------+-----------------+------------------+----------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+---------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, count, when\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Analysis\").getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = \"file:///home/talentum/myproject/dataSource/output/merged_final_csv/final.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 20 rows of the DataFrame\n",
    "print(\"Showing the first 20 rows:\")\n",
    "df.show(20)\n",
    "\n",
    "# Display the columns in the DataFrame\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Count the total number of rows in the DataFrame\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Count the number of null values in each column\n",
    "print(\"Number of null values in each column:\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Calculate the mean of each numeric column\n",
    "print(\"Mean of each numeric column:\")\n",
    "numeric_columns = [c for c, t in df.dtypes if t in ['int', 'double', 'float']]\n",
    "df.select([mean(col(c)).alias(c) for c in numeric_columns]).show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980a9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 32, Column names: ['at_degree', 'at_degree_c', 'benzene_ug_m3', 'bp_mmhg', 'co_mg_m3', 'eth_benzene_ug_m3', 'from_date', 'mp_xylene_ug_m3', 'nh3_ug_m3', 'no2_ug_m3', 'no_ug_m3', 'nox_ppb', 'nox_ug_m3', 'o_xylene_ug_m3', 'ozone_ug_m3', 'pm10_ug_m3', 'pm2_5_ug_m3', 'rf_mm', 'rh_%', 'rh_degree', 'so2_ug_m3', 'sr_w_mt2', 'state', 'temp_', 'temp_degree_c', 'to_date', 'toluene_ug_m3', 'vws_m_s', 'wd_deg', 'wd_degree', 'ws_m_s', 'xylene_ug_m3']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {len(df.columns)}, Column names: {df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42c4b4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e255233dc5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"at_degree\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         raise TypeError(\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "df.select(\"at_degree\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e7b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7ff72ec03470>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Analysis\").getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = \"file:///home/talentum/myproject/dataSource/output/merged_final_csv/final.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check if Spark session is active\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e71a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|at_degree|\n",
      "+---------+\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"at_degree\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd49a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in 'at_degree': 12108878\n",
      "Number of non-null values in 'at_degree': 10560\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Count the number of null values in the 'at_degree' column\n",
    "null_count = df.select(count(when(col(\"at_degree\").isNull(), \"at_degree\")).alias(\"null_count\")).collect()[0][\"null_count\"]\n",
    "\n",
    "# Count the number of non-null values in the 'at_degree' column\n",
    "non_null_count = df.select(count(col(\"at_degree\")).alias(\"non_null_count\")).collect()[0][\"non_null_count\"]\n",
    "\n",
    "print(f\"Number of null values in 'at_degree': {null_count}\")\n",
    "print(f\"Number of non-null values in 'at_degree': {non_null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a28b072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|at_degree_c|\n",
      "+-----------+\n",
      "|      21.47|\n",
      "|      27.97|\n",
      "|      22.37|\n",
      "|      26.47|\n",
      "|       24.0|\n",
      "|      23.37|\n",
      "|       22.0|\n",
      "|      22.27|\n",
      "|      23.05|\n",
      "|       21.6|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"at_degree_c\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dcc553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in 'at_degree': 12108878\n",
      "Number of non-null values in 'at_degree': 10560\n"
     ]
    }
   ],
   "source": [
    "null_count = df.select(count(when(col(\"at_degree\").isNull(), \"at_degree_c\")).alias(\"null_count\")).collect()[0][\"null_count\"]\n",
    "\n",
    "# Count the number of non-null values in the 'at_degree' column\n",
    "non_null_count = df.select(count(col(\"at_degree\")).alias(\"non_null_count\")).collect()[0][\"non_null_count\"]\n",
    "\n",
    "print(f\"Number of null values in 'at_degree': {null_count}\")\n",
    "print(f\"Number of non-null values in 'at_degree': {non_null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3e351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
