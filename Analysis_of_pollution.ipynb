{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd763dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5caab90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78cc0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Merger\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048b9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Andhra Pradesh': 'AP', 'Arunachal Pradesh': 'AR', 'Assam': 'AS', 'Bihar': 'BR', 'Chhattisgarh': 'CG', 'Chandigarh': 'CH', 'Delhi': 'DL', 'Gujarat': 'GJ', 'Himachal Pradesh': 'HP', 'Haryana': 'HR', 'Jharkhand': 'JH', 'Jammu and Kashmir': 'JK', 'Karnataka': 'KA', 'Kerala': 'KL', 'Maharashtra': 'MH', 'Meghalaya': 'ML', 'Manipur': 'MN', 'Madhya Pradesh': 'MP', 'Mizoram': 'MZ', 'Nagaland': 'NL', 'Odisha': 'OR', 'Punjab': 'PB', 'Puducherry': 'PY', 'Rajasthan': 'RJ', 'Sikkim': 'SK', 'Telangana': 'TG', 'Tamil Nadu': 'TN', 'Tripura': 'TR', 'Uttarakhand': 'UK', 'Uttar Pradesh': 'UP', 'West Bengal': 'WB'}\n"
     ]
    }
   ],
   "source": [
    "def create_dict_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['file_name'] = df['file_name'].str[:2]\n",
    "    result_dict = pd.Series(df['file_name'].values, index=df['state']).to_dict()\n",
    "    return result_dict\n",
    "\n",
    "# Example usage\n",
    "file_path = r'/home/talentum/myproject/dataSource/stations_info.csv'\n",
    "state_dict = create_dict_from_csv(file_path)\n",
    "print(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f0eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP', 'AR', 'AS', 'BR', 'CG', 'CH', 'DL', 'GJ', 'HP', 'HR', 'JH', 'JK', 'KA', 'KL', 'MH', 'ML', 'MN', 'MP', 'MZ', 'NL', 'OR', 'PB', 'PY', 'RJ', 'SK', 'TG', 'TN', 'TR', 'UK', 'UP', 'WB']\n"
     ]
    }
   ],
   "source": [
    "print(list(state_dict.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3febb",
   "metadata": {},
   "source": [
    "### merging cities with respect to states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc207296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVProcessor\").getOrCreate()\n",
    "\n",
    "# Define the directory paths\n",
    "directory_path = 'file:////home/talentum/myproject/dataSource/archive'\n",
    "savepath = 'file:////home/talentum/myproject/dataSource/output'\n",
    "\n",
    "# Define the prefixes you want to handle (assuming state_dict is defined elsewhere)\n",
    "prefixes = list(state_dict.values())  # Add other prefixes as needed\n",
    "\n",
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    normalized_columns = [col.strip().replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '') for col in df.columns]\n",
    "    return df.toDF(*normalized_columns)\n",
    "\n",
    "# Function to ensure correct data types\n",
    "def ensure_data_types(df, columns):\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            df = df.withColumn(column, lit(None).cast(StringType()))\n",
    "    return df.select(columns)\n",
    "\n",
    "# Process each prefix\n",
    "for prefix in prefixes:\n",
    "    # Define the search pattern\n",
    "    search_pattern = os.path.join(directory_path, f'{prefix}*.csv')\n",
    "    \n",
    "    # Read all CSV files using Spark's read method\n",
    "    df = spark.read.option(\"header\", \"true\").csv(search_pattern, inferSchema=True)\n",
    "    \n",
    "    if df.count() > 0:\n",
    "        # Normalize column names\n",
    "        df = normalize_column_names(df)\n",
    "        \n",
    "        # Identify all columns in the DataFrame\n",
    "        all_columns = sorted(df.columns)  # Sort columns for consistency\n",
    "        \n",
    "        # Ensure DataFrame has the correct columns and data types\n",
    "        df = ensure_data_types(df, all_columns)\n",
    "        \n",
    "        # Extract state abbreviation from file names (assuming prefix is the state abbreviation)\n",
    "        state_abbr = prefix\n",
    "        state_name = next((name for name, abbr in state_dict.items() if abbr == state_abbr), None)\n",
    "        \n",
    "        # Add the state column\n",
    "        df = df.withColumn('state', lit(state_name).cast(StringType()))\n",
    "        \n",
    "        # Save the DataFrame to a single CSV file\n",
    "        output_path = os.path.join(savepath, f'{prefix}')\n",
    "        df.coalesce(1).write.option(\"header\", \"true\").csv(output_path, mode='overwrite')\n",
    "        print(f\"Merged CSV files for prefix '{prefix}' saved as part csv file inside {prefix} folder'\")\n",
    "    else:\n",
    "        print(f\"No valid files found for prefix '{prefix}'.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c1553",
   "metadata": {},
   "source": [
    "### data cleaning(state wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, mean as _mean, to_date, count, isnull\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "def handle_outliers(df, column):\n",
    "    \"\"\"Handle outliers in a specific column using the IQR method.\"\"\"\n",
    "    Q1 = df.approxQuantile(column, [0.25], 0.01)[0]\n",
    "    Q3 = df.approxQuantile(column, [0.75], 0.01)[0]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df.withColumn(column, when((col(column) < lower_bound) | (col(column) > upper_bound), None).otherwise(col(column)))\n",
    "    median_value = df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "    df = df.fillna({column: median_value})\n",
    "    return df\n",
    "\n",
    "def process_csv_file(file_path, output_dir, state_code):\n",
    "    \"\"\"Read, clean, and save a single CSV file.\"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path, inferSchema=True)\n",
    "    \n",
    "    total_columns = len(df.columns)\n",
    "    missing_threshold_row = total_columns * 0.85\n",
    "    df = df.withColumn('missing_count', sum([isnull(col(c)).cast('int') for c in df.columns]))\n",
    "    df = df.filter(col('missing_count') <= missing_threshold_row).drop('missing_count')\n",
    "\n",
    "    # Step 2: Drop columns with more than 50% missing values (based on the new row count)\n",
    "    total_rows = df.count()\n",
    "    missing_threshold_col = total_rows * 0.60\n",
    "    missing_value_counts = {c: df.filter(isnull(col(c))).count() for c in df.columns}\n",
    "    cols_to_drop = [c for c in missing_value_counts if missing_value_counts[c] > missing_threshold_col]\n",
    "    df = df.drop(*cols_to_drop)\n",
    "    \n",
    "    # Handle outliers and impute missing values for numeric columns\n",
    "    numeric_columns = [f.name for f in df.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "    for column in numeric_columns:\n",
    "        df = handle_outliers(df, column)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    # Convert date columns\n",
    "    if 'From Date' in df.columns and 'To Date' in df.columns:\n",
    "        df = df.withColumn('From_Date', to_date(col('From Date'), 'yyyy-MM-dd'))\n",
    "        df = df.withColumn('To_Date', to_date(col('To Date'), 'yyyy-MM-dd'))\n",
    "        df = df.drop('From Date', 'To Date')\n",
    "    \n",
    "    # Normalize column names\n",
    "    normalized_columns = [col.strip().replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '').lower() for col in df.columns]\n",
    "    df = df.toDF(*normalized_columns)\n",
    "    \n",
    "    # Show the DataFrame and print remaining columns\n",
    "    df.show()\n",
    "    print(f\"Columns after cleaning: {df.columns}\")\n",
    "    \n",
    "    # Print a red warning if the number of columns is less than 12\n",
    "    if len(df.columns) < 12:\n",
    "        print(\"\\033[91mWarning: The number of columns after cleaning is less than 10.\\033[0m\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save the cleaned DataFrame to a single CSV file\n",
    "    output_file = os.path.join(output_dir, f'{state_code}')\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(f'file://{output_file}', mode='overwrite')\n",
    "    print(f'Cleaned data saved to {output_file}')\n",
    "\n",
    "def process_all_files(state_codes, base_path, output_dir):\n",
    "    \"\"\"Process CSV files from multiple state codes.\"\"\"\n",
    "    for state_code in state_codes:\n",
    "        file_path = os.path.join(base_path, state_code, '*.csv')\n",
    "        process_csv_file(file_path, output_dir, state_code)\n",
    "\n",
    "# Define state codes, base path, and output directory\n",
    "state_codes = list(state_dict.values())  # Example state codes\n",
    "# state_codes = ['JH']\n",
    "base_path = 'file:///home/talentum/myproject/dataSource/output'\n",
    "output_dir = '/home/talentum/myproject/dataSource/output/Cleaned'\n",
    "\n",
    "# Process all files\n",
    "process_all_files(state_codes, base_path, output_dir)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "# base_cleaned_file_path='hdfs:///user/talentum/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca968e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JH has less params\n",
    "#red color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c049fa4",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0739b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean as _mean, isnull\n",
    "from pyspark.sql.types import DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46a625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def process_csv_files_from_path(path_pattern):\n",
    "    \"\"\"Process all CSV files matching a path pattern.\"\"\"\n",
    "    try:\n",
    "        # Load all CSV files into a DataFrame using the path pattern\n",
    "        df = spark.read.option(\"header\", \"true\").csv(path_pattern, inferSchema=True)\n",
    "        \n",
    "        # Calculate the mean of each numeric column\n",
    "        numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (DoubleType, IntegerType))]\n",
    "        mean_values = df.select([_mean(col(column)).alias(column) for column in numeric_columns]).collect()[0].asDict()\n",
    "        \n",
    "        # Calculate the number of null values per column\n",
    "        null_counts = {column: df.filter(isnull(col(column))).count() for column in df.columns}\n",
    "        \n",
    "        # Total number of rows and columns\n",
    "        total_rows = df.count()\n",
    "        total_columns = len(df.columns)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Processing files matching: {path_pattern}\")\n",
    "        print(f\"Total Rows: {total_rows}\")\n",
    "        print(f\"Total Columns: {total_columns}\")\n",
    "        print(\"Mean of each feature column:\")\n",
    "        print(mean_values)\n",
    "        print(\"Number of null values per column:\")\n",
    "        print(null_counts)\n",
    "        print()  # Blank line for better readability\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing files with pattern {path_pattern}: {e}\")\n",
    "\n",
    "def process_files_for_state_code(state_code, base_paths):\n",
    "    \"\"\"Process all CSV files for a single state code across multiple base paths.\"\"\"\n",
    "    for base_path in base_paths: \n",
    "        print(\"******************************************************************************************\")\n",
    "        print(f\"Processing base path: {base_path} for state code: {state_code}\")\n",
    "        directory = os.path.join(base_path, state_code)\n",
    "        path_pattern = os.path.join(directory, '*.csv')  # Use wildcard to match all CSV files in the directory\n",
    "        process_csv_files_from_path(path_pattern)\n",
    "\n",
    "# Define base paths for directories\n",
    "base_paths = [\n",
    "    'file:///home/talentum/myproject/dataSource/output',\n",
    "    'file:///home/talentum/myproject/dataSource/output/Cleaned'\n",
    "]\n",
    "\n",
    "# Example state codes\n",
    "state_codes = list(state_dict.values())\n",
    "\n",
    "# Process all files for each state code across all base paths\n",
    "for state_code in state_codes:\n",
    "    process_files_for_state_code(state_code, base_paths)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73293628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OutlierDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def count_outliers_in_csv(file_path):\n",
    "    \"\"\"Count outliers in a given CSV file.\"\"\"\n",
    "    # Ensure the file path uses the correct prefix for local files\n",
    "    if not file_path.startswith('file:///'):\n",
    "        file_path = 'file://' + file_path\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path, inferSchema=True)\n",
    "\n",
    "    # Function to identify and count outliers using the IQR method\n",
    "    def count_outliers(df, column):\n",
    "        # Calculate Q1 and Q3 using approxQuantile function\n",
    "        quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "        q1, q3 = quantiles\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        # Count outliers\n",
    "        outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "        outlier_count = outliers.count()\n",
    "\n",
    "        return outlier_count\n",
    "\n",
    "    # Dictionary to store the count of outliers for each column\n",
    "    outlier_counts = {}\n",
    "\n",
    "    # Apply the outlier counting function to numeric columns only\n",
    "    for column in df.columns:\n",
    "        dtype = df.schema[column].dataType\n",
    "        if isinstance(dtype, (DoubleType, IntegerType)):\n",
    "            outlier_count = count_outliers(df, column)\n",
    "            outlier_counts[column] = outlier_count\n",
    "\n",
    "    # Print the results\n",
    "    for column, count in outlier_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"Column '{column}' has {count} outlier(s).\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' has no outliers.\")\n",
    "\n",
    "    return outlier_counts\n",
    "\n",
    "def process_csv_files(base_path, state_codes):\n",
    "    \"\"\"Process all CSV files for the given state codes.\"\"\"\n",
    "    for state_code in state_codes:\n",
    "        # Construct the directory path\n",
    "        print(\"*\"*20+f'{state_code}'+\"*\"*20)\n",
    "        directory_path = os.path.join(base_path, state_code)\n",
    "        print(f\"Looking for files in directory: {directory_path}\")  # Debugging line\n",
    "\n",
    "        # Construct the search pattern for CSV files\n",
    "        search_pattern = os.path.join(directory_path, '*.csv')\n",
    "        print(f\"Search pattern: {search_pattern}\")  # Debugging line\n",
    "\n",
    "        # List all CSV files in the directory\n",
    "        csv_files = glob.glob(search_pattern)\n",
    "\n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in directory: {directory_path}\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        for file_path in csv_files:\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            try:\n",
    "                outlier_counts = count_outliers_in_csv(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "state_codes = list(state_dict.values()) # Replace with actual state codes\n",
    "base_path = '/home/talentum/myproject/dataSource/output'  # Local base path\n",
    "\n",
    "process_csv_files(base_path, state_codes)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149e837",
   "metadata": {},
   "source": [
    "### save file as parquet after merging states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55603f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Merge\").getOrCreate()\n",
    "\n",
    "# List of state codes (assuming state_dict is defined elsewhere in your code)\n",
    "state_codes = list(state_dict.values())\n",
    "\n",
    "# Read the CSV files into a list of DataFrames\n",
    "dfs = []\n",
    "print(\"Starting to read CSV files...\")\n",
    "for state_code in state_codes:\n",
    "    file_path = f\"file:///home/talentum/myproject/dataSource/output/Cleaned/{state_code}/*.csv\"\n",
    "    print(f\"Reading files from: {file_path}\")\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    print(f\"Read {df.count()} rows for state code: {state_code}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "print(\"Finished reading CSV files.\")\n",
    "print(\"Starting to clean column names...\")\n",
    "\n",
    "# Function to clean column names by replacing special characters with underscores\n",
    "def clean_column_names(df):\n",
    "    for col_name in df.columns:\n",
    "        new_col_name = col_name.replace('-', '_').replace('/', '_').replace(' ', '_')\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "    return df\n",
    "\n",
    "# Clean column names for each DataFrame\n",
    "dfs = [clean_column_names(df) for df in dfs]\n",
    "\n",
    "print(\"Finished cleaning column names.\")\n",
    "print(\"Starting to ensure all DataFrames have the same columns...\")\n",
    "\n",
    "# Get the list of all columns\n",
    "all_columns = set()\n",
    "for df in dfs:\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Ensure all DataFrames have all columns\n",
    "def add_missing_columns(df, all_columns):\n",
    "    for column in all_columns:\n",
    "        if column not in df.columns:\n",
    "            df = df.withColumn(column, lit(None).cast(\"string\"))\n",
    "    return df\n",
    "\n",
    "dfs = [add_missing_columns(df, all_columns) for df in dfs]\n",
    "\n",
    "print(\"Finished ensuring all DataFrames have the same columns.\")\n",
    "print(\"Starting to select columns in the same order for consistency...\")\n",
    "\n",
    "# Select the columns in the same order for consistency\n",
    "dfs = [df.select(*all_columns) for df in dfs]\n",
    "\n",
    "print(\"Finished selecting columns in the same order.\")\n",
    "print(\"Starting to merge DataFrames...\")\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.unionByName(df)\n",
    "\n",
    "print(\"Finished merging DataFrames.\")\n",
    "print(\"Starting to save the merged DataFrame to a Parquet file...\")\n",
    "\n",
    "# Coalesce to a single partition and save the merged DataFrame to a Parquet file\n",
    "merged_df.coalesce(1).write.parquet(\"file:///home/talentum/myproject/dataSource/output/merged_final\")\n",
    "\n",
    "print(\"Finished saving the merged DataFrame as Parquet.\")\n",
    "print(\"Stopping the Spark session...\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2f781",
   "metadata": {},
   "source": [
    "### save as csv file according to coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e050961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, lit\n",
    "\n",
    "# # Initialize a Spark session\n",
    "# spark = SparkSession.builder.appName(\"CSV Merge\").getOrCreate()\n",
    "\n",
    "# # List of state codes (assuming state_dict is defined elsewhere in your code)\n",
    "# state_codes = list(state_dict.values())\n",
    "\n",
    "# # Read the CSV files into a list of DataFrames\n",
    "# dfs = []\n",
    "# print(\"Starting to read CSV files...\")\n",
    "# for state_code in state_codes:\n",
    "#     file_path = f\"file:///home/talentum/myproject/dataSource/output/Cleaned/{state_code}/*.csv\"\n",
    "#     print(f\"Reading files from: {file_path}\")\n",
    "#     df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "#     print(f\"Read {df.count()} rows for state code: {state_code}\")\n",
    "#     dfs.append(df)\n",
    "\n",
    "# print(\"Finished reading CSV files.\")\n",
    "# print(\"Starting to clean column names...\")\n",
    "\n",
    "# # Function to clean column names by replacing special characters with underscores\n",
    "# def clean_column_names(df):\n",
    "#     for col_name in df.columns:\n",
    "#         new_col_name = col_name.replace('-', '_').replace('/', '_').replace(' ', '_')\n",
    "#         df = df.withColumnRenamed(col_name, new_col_name)\n",
    "#     return df\n",
    "\n",
    "# # Clean column names for each DataFrame\n",
    "# dfs = [clean_column_names(df) for df in dfs]\n",
    "\n",
    "# print(\"Finished cleaning column names.\")\n",
    "# print(\"Starting to ensure all DataFrames have the same columns...\")\n",
    "\n",
    "# # Get the list of all columns\n",
    "# all_columns = set()\n",
    "# for df in dfs:\n",
    "#     all_columns.update(df.columns)\n",
    "\n",
    "# # Ensure all DataFrames have all columns\n",
    "# def add_missing_columns(df, all_columns):\n",
    "#     for column in all_columns:\n",
    "#         if column not in df.columns:\n",
    "#             df = df.withColumn(column, lit(None).cast(\"string\"))\n",
    "#     return df\n",
    "\n",
    "# dfs = [add_missing_columns(df, all_columns) for df in dfs]\n",
    "\n",
    "# print(\"Finished ensuring all DataFrames have the same columns.\")\n",
    "# print(\"Starting to select columns in the same order for consistency...\")\n",
    "\n",
    "# # Select the columns in the same order for consistency\n",
    "# dfs = [df.select(*all_columns) for df in dfs]\n",
    "\n",
    "# print(\"Finished selecting columns in the same order.\")\n",
    "# print(\"Starting to merge DataFrames...\")\n",
    "\n",
    "# # Merge the DataFrames\n",
    "# merged_df = dfs[0]\n",
    "# for df in dfs[1:]:\n",
    "#     merged_df = merged_df.unionByName(df)\n",
    "\n",
    "# print(\"Finished merging DataFrames.\")\n",
    "# print(\"Starting to save the merged DataFrame to a CSV file...\")\n",
    "\n",
    "# # Coalesce to a single partition and save the merged DataFrame to a CSV file\n",
    "# merged_df.coalesce(20).write.csv(\"file:///home/talentum/myproject/dataSource/output/merged_final_csv\", header=True)\n",
    "\n",
    "# print(\"Finished saving the merged DataFrame as CSV.\")\n",
    "# print(\"Stopping the Spark session...\")\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n",
    "\n",
    "# print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9f262",
   "metadata": {},
   "source": [
    "# Final cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f94ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bp_mmhg: string (nullable = true)\n",
      " |-- benzene_ug_m3: string (nullable = true)\n",
      " |-- mp_xylene_ug_m3: string (nullable = true)\n",
      " |-- o_xylene_ug_m3: string (nullable = true)\n",
      " |-- xylene_ug_m3: string (nullable = true)\n",
      " |-- co_mg_m3: string (nullable = true)\n",
      " |-- nh3_ug_m3: string (nullable = true)\n",
      " |-- so2_ug_m3: double (nullable = true)\n",
      " |-- at_degree: string (nullable = true)\n",
      " |-- rf_mm: string (nullable = true)\n",
      " |-- temp_degree_c: string (nullable = true)\n",
      " |-- from_date: string (nullable = true)\n",
      " |-- eth_benzene_ug_m3: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- temp_: string (nullable = true)\n",
      " |-- at_degree_c: string (nullable = true)\n",
      " |-- wd_degree: string (nullable = true)\n",
      " |-- wd_deg: string (nullable = true)\n",
      " |-- nox_ppb: string (nullable = true)\n",
      " |-- no_ug_m3: double (nullable = true)\n",
      " |-- toluene_ug_m3: string (nullable = true)\n",
      " |-- rh_degree: string (nullable = true)\n",
      " |-- nox_ug_m3: string (nullable = true)\n",
      " |-- to_date: string (nullable = true)\n",
      " |-- ozone_ug_m3: string (nullable = true)\n",
      " |-- pm10_ug_m3: string (nullable = true)\n",
      " |-- sr_w_mt2: string (nullable = true)\n",
      " |-- vws_m_s: string (nullable = true)\n",
      " |-- ws_m_s: string (nullable = true)\n",
      " |-- no2_ug_m3: double (nullable = true)\n",
      " |-- pm2_5_ug_m3: string (nullable = true)\n",
      " |-- rh_%: string (nullable = true)\n",
      "\n",
      "Columns overview\n",
      "+----------------+\n",
      "|       from_date|\n",
      "+----------------+\n",
      "|06-07-2016 22:00|\n",
      "|16-07-2016 12:00|\n",
      "|29-07-2016 17:00|\n",
      "|12-08-2016 15:00|\n",
      "|13-08-2016 23:00|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Shape of the dataset:  (12119438, 32)\n",
      "Null values in 'from_date': 0\n",
      "Null values in 'to_date': 0\n",
      "Let's print first 5 data rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bp_mmhg</th>\n",
       "      <th>benzene_ug_m3</th>\n",
       "      <th>mp_xylene_ug_m3</th>\n",
       "      <th>o_xylene_ug_m3</th>\n",
       "      <th>xylene_ug_m3</th>\n",
       "      <th>co_mg_m3</th>\n",
       "      <th>nh3_ug_m3</th>\n",
       "      <th>so2_ug_m3</th>\n",
       "      <th>at_degree</th>\n",
       "      <th>rf_mm</th>\n",
       "      <th>...</th>\n",
       "      <th>nox_ug_m3</th>\n",
       "      <th>to_date</th>\n",
       "      <th>ozone_ug_m3</th>\n",
       "      <th>pm10_ug_m3</th>\n",
       "      <th>sr_w_mt2</th>\n",
       "      <th>vws_m_s</th>\n",
       "      <th>ws_m_s</th>\n",
       "      <th>no2_ug_m3</th>\n",
       "      <th>pm2_5_ug_m3</th>\n",
       "      <th>rh_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>6.88</td>\n",
       "      <td>3.28</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>06-07-2016 23:00</td>\n",
       "      <td>13.78</td>\n",
       "      <td>12.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2.07</td>\n",
       "      <td>25.78</td>\n",
       "      <td>3.75</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.5</td>\n",
       "      <td>1.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.02</td>\n",
       "      <td>5.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>16-07-2016 13:00</td>\n",
       "      <td>15.5</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.03</td>\n",
       "      <td>32.47</td>\n",
       "      <td>12.33</td>\n",
       "      <td>59.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.47</td>\n",
       "      <td>6.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>29-07-2016 18:00</td>\n",
       "      <td>11.87</td>\n",
       "      <td>24.33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.23</td>\n",
       "      <td>51.73</td>\n",
       "      <td>4.0</td>\n",
       "      <td>74.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5.00</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>12-08-2016 16:00</td>\n",
       "      <td>9.15</td>\n",
       "      <td>89.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>61.20</td>\n",
       "      <td>18.75</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>218.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>6.73</td>\n",
       "      <td>7.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>14-08-2016 00:00</td>\n",
       "      <td>17.85</td>\n",
       "      <td>75.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.33</td>\n",
       "      <td>55.72</td>\n",
       "      <td>29.25</td>\n",
       "      <td>65.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  bp_mmhg benzene_ug_m3 mp_xylene_ug_m3 o_xylene_ug_m3 xylene_ug_m3 co_mg_m3  \\\n",
       "0   218.5           0.4            None           None          0.1     0.26   \n",
       "1   218.5          1.13            None           None          0.5     0.46   \n",
       "2   218.5           0.7            None           None          0.1     0.55   \n",
       "3   218.5          0.85            None           None          0.1     0.57   \n",
       "4   218.5           1.0            None           None          0.1     0.27   \n",
       "\n",
       "  nh3_ug_m3  so2_ug_m3 at_degree rf_mm  ... nox_ug_m3           to_date  \\\n",
       "0      6.88       3.28      None   0.0  ...      None  06-07-2016 23:00   \n",
       "1     10.02       5.33      None   0.0  ...      None  16-07-2016 13:00   \n",
       "2      8.47       6.20      None   0.0  ...      None  29-07-2016 18:00   \n",
       "3       7.7       5.00      None   0.0  ...      None  12-08-2016 16:00   \n",
       "4      6.73       7.25      None   0.0  ...      None  14-08-2016 00:00   \n",
       "\n",
       "  ozone_ug_m3 pm10_ug_m3 sr_w_mt2 vws_m_s ws_m_s no2_ug_m3 pm2_5_ug_m3   rh_%  \n",
       "0       13.78       12.5     10.0    -0.1   2.07     25.78        3.75  78.75  \n",
       "1        15.5       47.0      3.0     0.2   1.03     32.47       12.33  59.33  \n",
       "2       11.87      24.33      3.0    0.27   1.23     51.73         4.0  74.33  \n",
       "3        9.15      89.25      3.0     1.0   1.12     61.20       18.75   59.5  \n",
       "4       17.85       75.5      6.0    0.23   1.33     55.72       29.25  65.25  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, to_date, col\n",
    "from pyspark.sql.types import StringType\n",
    "from math import ceil\n",
    "from pyspark import StorageLevel\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/merged_final/*.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "df.printSchema()\n",
    "print('Columns overview')\n",
    "pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])\n",
    "df.select(\"from_date\").show(5)\n",
    "# Shape of the dataset\n",
    "print('Shape of the dataset: ', (df.count(), len(df.columns)))\n",
    "\n",
    "from_date_null_count = df.filter(col(\"from_date\").isNull()).count()\n",
    "to_date_null_count = df.filter(col(\"to_date\").isNull()).count()\n",
    "\n",
    "print(f\"Null values in 'from_date': {from_date_null_count}\")\n",
    "print(f\"Null values in 'to_date': {to_date_null_count}\")\n",
    "# Stop the Spark session\n",
    "# df = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"Let's print first 5 data rows:\")\n",
    "df.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf436911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Numerial columns:\n",
      "['so2_ug_m3', 'no_ug_m3', 'no2_ug_m3']\n",
      "\n",
      "\n",
      "All string columns:\n",
      "['bp_mmhg', 'benzene_ug_m3', 'mp_xylene_ug_m3', 'o_xylene_ug_m3', 'xylene_ug_m3', 'co_mg_m3', 'nh3_ug_m3', 'at_degree', 'rf_mm', 'temp_degree_c', 'eth_benzene_ug_m3', 'state', 'temp_', 'at_degree_c', 'wd_degree', 'wd_deg', 'nox_ppb', 'toluene_ug_m3', 'rh_degree', 'nox_ug_m3', 'ozone_ug_m3', 'pm10_ug_m3', 'sr_w_mt2', 'vws_m_s', 'ws_m_s', 'pm2_5_ug_m3', 'rh_%']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all column names and their data types\n",
    "column_info = df.dtypes\n",
    "\n",
    "# Filter the list to only include numerical columns\n",
    "numerical_columns = [col[0] for col in column_info if col[1] in (\"IntegerType\", \"double\")]\n",
    "string_columns = [col[0] for col in column_info if col[1] == \"string\"]\n",
    "\n",
    "print(\"All Numerial columns:\")\n",
    "print(numerical_columns)\n",
    "print(\"\\n\")\n",
    "print(\"All string columns:\")\n",
    "print(string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e92d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import rand, to_date, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79aeb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume 'df' is your PySpark DataFrame and 'source' is your column name\n",
    "df = df.withColumn(\"from_date\", to_date(col(\"from_date\"), \"yyyy-MM-dd\"))\\\n",
    "        .withColumn(\"to_date\", to_date(col(\"to_date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf992a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b611f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Null values in 'from_date': 102938\n",
      "Null values in 'to_date': 102938\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(df.columns))\n",
    "from_date_null_count = df.filter(col(\"from_date\").isNull()).count()\n",
    "to_date_null_count = df.filter(col(\"to_date\").isNull()).count()\n",
    "\n",
    "print(f\"Null values in 'from_date': {from_date_null_count}\")\n",
    "print(f\"Null values in 'to_date': {to_date_null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bce52a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'from_date': 0\n",
      "Null values in 'to_date': 0\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'from_date' or 'to_date' is null\n",
    "df = df.dropna(subset=[\"from_date\", \"to_date\"])\n",
    "\n",
    "from_date_null_count = df.filter(col(\"from_date\").isNull()).count()\n",
    "to_date_null_count = df.filter(col(\"to_date\").isNull()).count()\n",
    "\n",
    "print(f\"Null values in 'from_date': {from_date_null_count}\")\n",
    "print(f\"Null values in 'to_date': {to_date_null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92300de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_cast = [c for c in df.columns if c not in ['state','from_date','to_date']]\n",
    "for c in columns_to_cast:\n",
    "    df = df.withColumn(c, col(c).cast(\"double\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acca2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Numerial columns:\n",
      "['bp_mmhg', 'benzene_ug_m3', 'mp_xylene_ug_m3', 'o_xylene_ug_m3', 'xylene_ug_m3', 'co_mg_m3', 'nh3_ug_m3', 'so2_ug_m3', 'at_degree', 'rf_mm', 'temp_degree_c', 'eth_benzene_ug_m3', 'temp_', 'at_degree_c', 'wd_degree', 'wd_deg', 'nox_ppb', 'no_ug_m3', 'toluene_ug_m3', 'rh_degree', 'nox_ug_m3', 'ozone_ug_m3', 'pm10_ug_m3', 'sr_w_mt2', 'vws_m_s', 'ws_m_s', 'no2_ug_m3', 'pm2_5_ug_m3', 'rh_%']\n",
      "\n",
      "\n",
      "All string columns:\n",
      "['state']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns_new = [col[0] for col in df.dtypes if col[1] in (\"IntegerType\", \"double\")]\n",
    "string_columns_new = [col[0] for col in df.dtypes if col[1] == \"string\"]\n",
    "\n",
    "print(\"All Numerial columns:\")\n",
    "print(numerical_columns_new)\n",
    "print(\"\\n\")\n",
    "print(\"All string columns:\")\n",
    "print(string_columns_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea162d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bp_mmhg: double (nullable = true)\n",
      " |-- benzene_ug_m3: double (nullable = true)\n",
      " |-- mp_xylene_ug_m3: double (nullable = true)\n",
      " |-- o_xylene_ug_m3: double (nullable = true)\n",
      " |-- xylene_ug_m3: double (nullable = true)\n",
      " |-- co_mg_m3: double (nullable = true)\n",
      " |-- nh3_ug_m3: double (nullable = true)\n",
      " |-- so2_ug_m3: double (nullable = true)\n",
      " |-- at_degree: double (nullable = true)\n",
      " |-- rf_mm: double (nullable = true)\n",
      " |-- temp_degree_c: double (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- eth_benzene_ug_m3: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- temp_: double (nullable = true)\n",
      " |-- at_degree_c: double (nullable = true)\n",
      " |-- wd_degree: double (nullable = true)\n",
      " |-- wd_deg: double (nullable = true)\n",
      " |-- nox_ppb: double (nullable = true)\n",
      " |-- no_ug_m3: double (nullable = true)\n",
      " |-- toluene_ug_m3: double (nullable = true)\n",
      " |-- rh_degree: double (nullable = true)\n",
      " |-- nox_ug_m3: double (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- ozone_ug_m3: double (nullable = true)\n",
      " |-- pm10_ug_m3: double (nullable = true)\n",
      " |-- sr_w_mt2: double (nullable = true)\n",
      " |-- vws_m_s: double (nullable = true)\n",
      " |-- ws_m_s: double (nullable = true)\n",
      " |-- no2_ug_m3: double (nullable = true)\n",
      " |-- pm2_5_ug_m3: double (nullable = true)\n",
      " |-- rh_%: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81b8ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| from_date|\n",
      "+----------+\n",
      "|2017-07-03|\n",
      "|2017-07-07|\n",
      "|2017-07-08|\n",
      "|2017-07-15|\n",
      "|2017-07-17|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"from_date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d961b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            state|\n",
      "+-----------------+\n",
      "|         Nagaland|\n",
      "|        Karnataka|\n",
      "|           Odisha|\n",
      "|           Kerala|\n",
      "|       Tamil Nadu|\n",
      "|     Chhattisgarh|\n",
      "|   Andhra Pradesh|\n",
      "|   Madhya Pradesh|\n",
      "|           Punjab|\n",
      "|          Manipur|\n",
      "|          Mizoram|\n",
      "| Himachal Pradesh|\n",
      "|       Puducherry|\n",
      "|          Haryana|\n",
      "|Jammu and Kashmir|\n",
      "|        Jharkhand|\n",
      "|Arunachal Pradesh|\n",
      "|          Gujarat|\n",
      "|           Sikkim|\n",
      "|            Delhi|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"state\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3afd2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct states: 31\n"
     ]
    }
   ],
   "source": [
    "distinct_state_count = df.select(\"state\").distinct().count()\n",
    "print(f\"Number of distinct states: {distinct_state_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f103015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|state            |count  |\n",
      "+-----------------+-------+\n",
      "|Andhra Pradesh   |122778 |\n",
      "|Arunachal Pradesh|8131   |\n",
      "|Assam            |73172  |\n",
      "|Bihar            |636501 |\n",
      "|Chandigarh       |54786  |\n",
      "|Chhattisgarh     |49048  |\n",
      "|Delhi            |2118037|\n",
      "|Gujarat          |346017 |\n",
      "|Haryana          |1110835|\n",
      "|Himachal Pradesh |8851   |\n",
      "|Jammu and Kashmir|16792  |\n",
      "|Jharkhand        |52624  |\n",
      "|Karnataka        |1087206|\n",
      "|Kerala           |264335 |\n",
      "|Madhya Pradesh   |572106 |\n",
      "|Maharashtra      |1264555|\n",
      "|Manipur          |14918  |\n",
      "|Meghalaya        |30422  |\n",
      "|Mizoram          |24991  |\n",
      "|Nagaland         |21414  |\n",
      "|Odisha           |102796 |\n",
      "|Puducherry       |19109  |\n",
      "|Punjab           |374848 |\n",
      "|Rajasthan        |485589 |\n",
      "|Sikkim           |8048   |\n",
      "|Tamil Nadu       |518525 |\n",
      "|Telangana        |390250 |\n",
      "|Tripura          |29013  |\n",
      "|Uttar Pradesh    |1641091|\n",
      "|Uttarakhand      |10560  |\n",
      "|West Bengal      |559152 |\n",
      "+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").count().orderBy(\"state\").show(31,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d0233aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0966250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bp_mmhg':'2268611'\n",
      "'benzene_ug_m3':'393359'\n",
      "'mp_xylene_ug_m3':'5255939'\n",
      "'o_xylene_ug_m3':'5598049'\n",
      "'xylene_ug_m3':'8980485'\n",
      "'co_mg_m3':'52624'\n",
      "'nh3_ug_m3':'3807766'\n",
      "'so2_ug_m3':'0'\n",
      "'at_degree':'12005940'\n",
      "'rf_mm':'10184848'\n",
      "'temp_degree_c':'3618911'\n",
      "'eth_benzene_ug_m3':'8095438'\n",
      "'state':'0'\n",
      "'temp_':'10820847'\n",
      "'at_degree_c':'7084985'\n",
      "'wd_degree':'10985297'\n",
      "'wd_deg':'1083827'\n",
      "'nox_ppb':'155420'\n",
      "'no_ug_m3':'0'\n",
      "'toluene_ug_m3':'556254'\n",
      "'rh_degree':'11913704'\n",
      "'nox_ug_m3':'11913704'\n",
      "'ozone_ug_m3':'49048'\n",
      "'pm10_ug_m3':'2137586'\n",
      "'sr_w_mt2':'1861137'\n",
      "'vws_m_s':'6820905'\n",
      "'ws_m_s':'52624'\n",
      "'no2_ug_m3':'0'\n",
      "'pm2_5_ug_m3':'52624'\n",
      "'rh_%':'155420'\n"
     ]
    }
   ],
   "source": [
    "for col_name in df1.columns:\n",
    "    if col_name not in [\"to_date\", \"from_date\"]:\n",
    "        null_count = df1.filter(col(col_name).isNull()).count()\n",
    "        print(f\"'{col_name}':'{null_count}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aaf4f93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>((bp_mmhg / 12016500) * 100)</th>\n",
       "      <td>18.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((benzene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>3.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((mp_xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>43.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((o_xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>46.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>74.73%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((co_mg_m3 / 12016500) * 100)</th>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nh3_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>31.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((so2_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((at_degree / 12016500) * 100)</th>\n",
       "      <td>99.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rf_mm / 12016500) * 100)</th>\n",
       "      <td>84.76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((temp_degree_c / 12016500) * 100)</th>\n",
       "      <td>30.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((eth_benzene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>67.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((state / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((temp_ / 12016500) * 100)</th>\n",
       "      <td>90.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((at_degree_c / 12016500) * 100)</th>\n",
       "      <td>58.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((wd_degree / 12016500) * 100)</th>\n",
       "      <td>91.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((wd_deg / 12016500) * 100)</th>\n",
       "      <td>9.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nox_ppb / 12016500) * 100)</th>\n",
       "      <td>1.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((no_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((toluene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>4.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rh_degree / 12016500) * 100)</th>\n",
       "      <td>99.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nox_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>99.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((ozone_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((pm10_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>17.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((sr_w_mt2 / 12016500) * 100)</th>\n",
       "      <td>15.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((vws_m_s / 12016500) * 100)</th>\n",
       "      <td>56.76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((ws_m_s / 12016500) * 100)</th>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((no2_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((pm2_5_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rh_% / 12016500) * 100)</th>\n",
       "      <td>1.29%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Null Percentage\n",
       "((bp_mmhg / 12016500) * 100)                    18.88%\n",
       "((benzene_ug_m3 / 12016500) * 100)               3.27%\n",
       "((mp_xylene_ug_m3 / 12016500) * 100)            43.74%\n",
       "((o_xylene_ug_m3 / 12016500) * 100)             46.59%\n",
       "((xylene_ug_m3 / 12016500) * 100)               74.73%\n",
       "((co_mg_m3 / 12016500) * 100)                    0.44%\n",
       "((nh3_ug_m3 / 12016500) * 100)                  31.69%\n",
       "((so2_ug_m3 / 12016500) * 100)                   0.00%\n",
       "((at_degree / 12016500) * 100)                  99.91%\n",
       "((rf_mm / 12016500) * 100)                      84.76%\n",
       "((temp_degree_c / 12016500) * 100)              30.12%\n",
       "((eth_benzene_ug_m3 / 12016500) * 100)          67.37%\n",
       "((state / 12016500) * 100)                       0.00%\n",
       "((temp_ / 12016500) * 100)                      90.05%\n",
       "((at_degree_c / 12016500) * 100)                58.96%\n",
       "((wd_degree / 12016500) * 100)                  91.42%\n",
       "((wd_deg / 12016500) * 100)                      9.02%\n",
       "((nox_ppb / 12016500) * 100)                     1.29%\n",
       "((no_ug_m3 / 12016500) * 100)                    0.00%\n",
       "((toluene_ug_m3 / 12016500) * 100)               4.63%\n",
       "((rh_degree / 12016500) * 100)                  99.14%\n",
       "((nox_ug_m3 / 12016500) * 100)                  99.14%\n",
       "((ozone_ug_m3 / 12016500) * 100)                 0.41%\n",
       "((pm10_ug_m3 / 12016500) * 100)                 17.79%\n",
       "((sr_w_mt2 / 12016500) * 100)                   15.49%\n",
       "((vws_m_s / 12016500) * 100)                    56.76%\n",
       "((ws_m_s / 12016500) * 100)                      0.44%\n",
       "((no2_ug_m3 / 12016500) * 100)                   0.00%\n",
       "((pm2_5_ug_m3 / 12016500) * 100)                 0.44%\n",
       "((rh_% / 12016500) * 100)                        1.29%"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# List of columns to ignore\n",
    "ignore_columns = [\"to_date\", \"from_date\"]\n",
    "\n",
    "# Filter out the ignored columns\n",
    "filtered_columns = [c for c in df1.columns if c not in ignore_columns]\n",
    "\n",
    "total_count = df1.count()\n",
    "\n",
    "# Calculate null counts for the filtered columns\n",
    "null_counts = df1.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in filtered_columns])\n",
    "\n",
    "# Calculate null percentage for the filtered columns\n",
    "null_counts_percent = null_counts.select([(col(c) / total_count) * 100 for c in null_counts.columns])\n",
    "\n",
    "# Convert to Pandas DataFrame and transpose\n",
    "null_counts_percent_pd = null_counts_percent.toPandas().transpose()\n",
    "\n",
    "# Rename the column and format the percentages\n",
    "null_counts_percent_pd.columns = ['Null Percentage']\n",
    "null_counts_percent_pd['Null Percentage'] = null_counts_percent_pd['Null Percentage'].apply(lambda x: '{:.2f}%'.format(x))\n",
    "\n",
    "null_counts_percent_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06ca5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((xylene_ug_m3 / 12016500) * 100)\n",
      "((at_degree / 12016500) * 100)\n",
      "((rf_mm / 12016500) * 100)\n",
      "((eth_benzene_ug_m3 / 12016500) * 100)\n",
      "((temp_ / 12016500) * 100)\n",
      "((at_degree_c / 12016500) * 100)\n",
      "((wd_degree / 12016500) * 100)\n",
      "((rh_degree / 12016500) * 100)\n",
      "((nox_ug_m3 / 12016500) * 100)\n",
      "((vws_m_s / 12016500) * 100)\n",
      "Columns dropped: ['xylene_ug_m3', 'at_degree', 'rf_mm', 'eth_benzene_ug_m3', 'temp_', 'at_degree_c', 'wd_degree', 'rh_degree', 'nox_ug_m3', 'vws_m_s']\n",
      "\n",
      "\n",
      "New DataFrame shape: 12016500 x 22\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Columns to ignore\n",
    "ignore_columns = [\"to_date\", \"from_date\", \"state\"]\n",
    "\n",
    "# Check if 'Null Percentage' contains strings and remove '%' symbol if necessary\n",
    "if null_counts_percent_pd['Null Percentage'].dtype == 'object':\n",
    "    null_counts_percent_pd['Null Percentage'] = null_counts_percent_pd['Null Percentage'].str.rstrip('%').astype(float)\n",
    "\n",
    "# Filter the DataFrame to exclude the ignored columns\n",
    "filtered_null_counts_percent_pd = null_counts_percent_pd.drop(ignore_columns, errors='ignore')\n",
    "\n",
    "# Filter the columns with 'Null Percentage' more than 50%\n",
    "high_null_columns = filtered_null_counts_percent_pd[filtered_null_counts_percent_pd['Null Percentage'] > 50]\n",
    "\n",
    "# Print the column names\n",
    "for column in high_null_columns.index:\n",
    "    print(column)\n",
    "\n",
    "\n",
    "\n",
    "# Drop the columns\n",
    "#columns_to_drop1 = [re.search(r'([^\\/]+)', col).group(1).strip().replace('(', '').replace(')', '') for col in columns_to_drop]\n",
    "columns_to_drop1 = ['xylene_ug_m3', 'at_degree', 'rf_mm', 'eth_benzene_ug_m3', 'temp_', 'at_degree_c', 'wd_degree', 'rh_degree', 'nox_ug_m3', 'vws_m_s']\n",
    "df1_dropped = df1.drop(*columns_to_drop1)\n",
    "\n",
    "print(\"Columns dropped:\", columns_to_drop1)\n",
    "print(\"\\n\")\n",
    "print(\"New DataFrame shape:\", df1_dropped.count(), \"x\", len(df1_dropped.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "225cdec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns dropped = 10\n",
      "original number of columns available = 32\n"
     ]
    }
   ],
   "source": [
    "print(\"number of columns dropped = {}\".format(len(columns_to_drop1)))\n",
    "print(\"original number of columns available = {}\".format(len(df1.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38147927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bp_mmhg: double (nullable = true)\n",
      " |-- benzene_ug_m3: double (nullable = true)\n",
      " |-- mp_xylene_ug_m3: double (nullable = true)\n",
      " |-- o_xylene_ug_m3: double (nullable = true)\n",
      " |-- co_mg_m3: double (nullable = true)\n",
      " |-- nh3_ug_m3: double (nullable = true)\n",
      " |-- so2_ug_m3: double (nullable = true)\n",
      " |-- temp_degree_c: double (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- wd_deg: double (nullable = true)\n",
      " |-- nox_ppb: double (nullable = true)\n",
      " |-- no_ug_m3: double (nullable = true)\n",
      " |-- toluene_ug_m3: double (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- ozone_ug_m3: double (nullable = true)\n",
      " |-- pm10_ug_m3: double (nullable = true)\n",
      " |-- sr_w_mt2: double (nullable = true)\n",
      " |-- ws_m_s: double (nullable = true)\n",
      " |-- no2_ug_m3: double (nullable = true)\n",
      " |-- pm2_5_ug_m3: double (nullable = true)\n",
      " |-- rh_%: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_dropped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad8a1a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped columns \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>((xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>74.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((at_degree / 12016500) * 100)</th>\n",
       "      <td>99.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rf_mm / 12016500) * 100)</th>\n",
       "      <td>84.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((eth_benzene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>67.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((temp_ / 12016500) * 100)</th>\n",
       "      <td>90.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((at_degree_c / 12016500) * 100)</th>\n",
       "      <td>58.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((wd_degree / 12016500) * 100)</th>\n",
       "      <td>91.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rh_degree / 12016500) * 100)</th>\n",
       "      <td>99.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nox_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>99.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((vws_m_s / 12016500) * 100)</th>\n",
       "      <td>56.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Null Percentage\n",
       "((xylene_ug_m3 / 12016500) * 100)                 74.73\n",
       "((at_degree / 12016500) * 100)                    99.91\n",
       "((rf_mm / 12016500) * 100)                        84.76\n",
       "((eth_benzene_ug_m3 / 12016500) * 100)            67.37\n",
       "((temp_ / 12016500) * 100)                        90.05\n",
       "((at_degree_c / 12016500) * 100)                  58.96\n",
       "((wd_degree / 12016500) * 100)                    91.42\n",
       "((rh_degree / 12016500) * 100)                    99.14\n",
       "((nox_ug_m3 / 12016500) * 100)                    99.14\n",
       "((vws_m_s / 12016500) * 100)                      56.76"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"dropped columns \")\n",
    "null_counts_percent_pd[null_counts_percent_pd['Null Percentage'] > 50.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # assume 'df' is your PySpark DataFrame\n",
    "\n",
    "# # convert the DataFrame to a Pandas DataFrame\n",
    "# pdf = df1_dropped.limit(800000).toPandas()\n",
    "\n",
    "# # iterate over each column and create a density plot, ignoring string columns\n",
    "# for col in pdf.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(pdf[col]):\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         sns.kdeplot(pdf[col], shade=True)\n",
    "#         plt.title(f\"Density Plot of {col}\")\n",
    "#         plt.xlabel(col)\n",
    "#         plt.ylabel(\"Density\")\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "183f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "numeric_column_names = [column.name for column in df1_dropped.schema.fields\n",
    "                        if isinstance(column.dataType, (IntegerType, FloatType, DoubleType))]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols= numeric_column_names, #specifying the input column names\n",
    "    outputCols=numeric_column_names, #specifying the output column names\n",
    "    strategy=\"median\"                  # or \"median\" if you want to use the median value\n",
    ")\n",
    "\n",
    "# Fit the Imputer\n",
    "model = imputer.fit(df1_dropped)\n",
    "\n",
    "#Transform the dataset\n",
    "imputed_df = model.transform(df1_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6122597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bp_mmhg',\n",
       " 'benzene_ug_m3',\n",
       " 'mp_xylene_ug_m3',\n",
       " 'o_xylene_ug_m3',\n",
       " 'co_mg_m3',\n",
       " 'nh3_ug_m3',\n",
       " 'so2_ug_m3',\n",
       " 'temp_degree_c',\n",
       " 'wd_deg',\n",
       " 'nox_ppb',\n",
       " 'no_ug_m3',\n",
       " 'toluene_ug_m3',\n",
       " 'ozone_ug_m3',\n",
       " 'pm10_ug_m3',\n",
       " 'sr_w_mt2',\n",
       " 'ws_m_s',\n",
       " 'no2_ug_m3',\n",
       " 'pm2_5_ug_m3',\n",
       " 'rh_%']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "791680a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>((bp_mmhg / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((benzene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((mp_xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((o_xylene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((co_mg_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nh3_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((so2_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((temp_degree_c / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((from_date / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((state / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((wd_deg / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((nox_ppb / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((no_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((toluene_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((to_date / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((ozone_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((pm10_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((sr_w_mt2 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((ws_m_s / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((no2_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((pm2_5_ug_m3 / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>((rh_% / 12016500) * 100)</th>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Null Percentage\n",
       "((bp_mmhg / 12016500) * 100)                   0.00%\n",
       "((benzene_ug_m3 / 12016500) * 100)             0.00%\n",
       "((mp_xylene_ug_m3 / 12016500) * 100)           0.00%\n",
       "((o_xylene_ug_m3 / 12016500) * 100)            0.00%\n",
       "((co_mg_m3 / 12016500) * 100)                  0.00%\n",
       "((nh3_ug_m3 / 12016500) * 100)                 0.00%\n",
       "((so2_ug_m3 / 12016500) * 100)                 0.00%\n",
       "((temp_degree_c / 12016500) * 100)             0.00%\n",
       "((from_date / 12016500) * 100)                 0.00%\n",
       "((state / 12016500) * 100)                     0.00%\n",
       "((wd_deg / 12016500) * 100)                    0.00%\n",
       "((nox_ppb / 12016500) * 100)                   0.00%\n",
       "((no_ug_m3 / 12016500) * 100)                  0.00%\n",
       "((toluene_ug_m3 / 12016500) * 100)             0.00%\n",
       "((to_date / 12016500) * 100)                   0.00%\n",
       "((ozone_ug_m3 / 12016500) * 100)               0.00%\n",
       "((pm10_ug_m3 / 12016500) * 100)                0.00%\n",
       "((sr_w_mt2 / 12016500) * 100)                  0.00%\n",
       "((ws_m_s / 12016500) * 100)                    0.00%\n",
       "((no2_ug_m3 / 12016500) * 100)                 0.00%\n",
       "((pm2_5_ug_m3 / 12016500) * 100)               0.00%\n",
       "((rh_% / 12016500) * 100)                      0.00%"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether null values are still present or not\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "total_count = imputed_df.count()\n",
    "null_counts = imputed_df.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in imputed_df.columns])\n",
    "\n",
    "null_counts_percent = null_counts.select([(col(c) / total_count) * 100 for c in null_counts.columns])\n",
    "\n",
    "null_counts_percent_pd = null_counts_percent.toPandas().transpose()\n",
    "\n",
    "null_counts_percent_pd.columns = ['Null Percentage']\n",
    "null_counts_percent_pd['Null Percentage'] = null_counts_percent_pd['Null Percentage'].apply(lambda x: '{:.2f}%'.format(x))\n",
    "\n",
    "null_counts_percent_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6dbde37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| from_date|\n",
      "+----------+\n",
      "|2017-07-03|\n",
      "|2017-07-07|\n",
      "|2017-07-08|\n",
      "|2017-07-15|\n",
      "|2017-07-17|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputed_df.select(\"from_date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a455a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "844ce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.coalesce(1).write.parquet(\"file:///home/talentum/myproject/dataSource/output/final_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37ddc2",
   "metadata": {},
   "source": [
    "### final check for cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47450ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+\n",
      "|mp_xylene_ug_m3|o_xylene_ug_m3|co_mg_m3|nh3_ug_m3|so2_ug_m3|temp_degree_c| from_date|         state|wd_deg|nox_ppb|\n",
      "+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+\n",
      "|           2.19|          2.65|    0.87|      3.8|     20.2|         27.0|2017-07-03|Andhra Pradesh|297.75|  12.13|\n",
      "|           2.19|          2.65|     0.0|     3.65|     20.2|        27.45|2017-07-07|Andhra Pradesh|288.75|  13.12|\n",
      "|           2.19|          2.65|     0.0|      3.2|     6.85|         27.0|2017-07-08|Andhra Pradesh| 288.5|   8.72|\n",
      "|           2.19|          2.65|     0.0|     3.48|     2.62|        29.63|2017-07-15|Andhra Pradesh| 295.5|    6.0|\n",
      "|           2.19|          2.65|     0.0|     2.38|     7.75|        44.43|2017-07-17|Andhra Pradesh| 299.5|   8.63|\n",
      "|           2.19|          2.65|    0.01|     4.13|    18.03|        27.83|2017-07-24|Andhra Pradesh| 286.0|    5.6|\n",
      "|           2.19|          2.65|     0.0|      2.5|    13.95|        27.87|2017-08-16|Andhra Pradesh| 307.5|   9.65|\n",
      "|           2.19|          2.65|     0.0|     6.93|    20.92|        36.13|2017-08-20|Andhra Pradesh| 299.5|  11.48|\n",
      "|           2.19|          2.65|     0.0|     2.72|     15.1|        28.47|2017-08-25|Andhra Pradesh|199.75|   4.77|\n",
      "|           2.19|          2.65|     0.0|    10.02|     7.75|         55.6|2017-08-27|Andhra Pradesh| 249.5|  11.72|\n",
      "|           2.19|          2.65|     0.0|      1.9|     2.73|        27.77|2017-09-01|Andhra Pradesh|  81.0|    8.1|\n",
      "|           2.19|          2.65|     0.0|     1.17|     5.27|        28.93|2017-09-11|Andhra Pradesh| 191.0|   3.87|\n",
      "|           2.19|          2.65|     0.0|      3.3|     7.75|        28.48|2017-09-15|Andhra Pradesh| 249.5|   5.08|\n",
      "|           2.19|          2.65|     0.0|     3.32|     5.32|        27.92|2017-09-23|Andhra Pradesh|  86.0|   6.28|\n",
      "|           2.19|          2.65|     0.0|      1.9|     8.75|         29.0|2017-09-25|Andhra Pradesh| 275.0|    2.3|\n",
      "|           2.19|          2.65|     0.0|     0.17|     0.63|         28.2|2017-09-27|Andhra Pradesh|  30.0|   0.63|\n",
      "|           2.19|          2.65|     0.9|     0.23|     0.63|        27.83|2017-09-28|Andhra Pradesh|  45.0|    0.3|\n",
      "|           2.19|          2.65|    1.14|     0.08|     7.75|        32.32|2017-09-30|Andhra Pradesh| 171.0|   0.92|\n",
      "|           2.19|          2.65|    0.57|     0.43|    16.77|         31.9|2017-09-30|Andhra Pradesh| 190.0|    0.7|\n",
      "|           2.19|          2.65|     0.0|     0.03|     7.75|        28.62|2017-10-14|Andhra Pradesh|  86.0|    0.0|\n",
      "|           2.19|          2.65|    0.88|     0.13|    19.87|        17.57|2017-10-19|Andhra Pradesh|101.33|   0.03|\n",
      "|           2.19|          2.65|     0.5|    10.02|     4.92|         21.3|2017-11-04|Andhra Pradesh|  92.0|   0.45|\n",
      "|           2.19|          2.65|     0.7|      0.4|     4.13|        28.97|2017-11-13|Andhra Pradesh|155.33|   4.87|\n",
      "|           2.19|          2.65|    0.63|     0.92|     4.73|        29.73|2017-11-18|Andhra Pradesh|  53.0|   9.32|\n",
      "|           2.19|          2.65|    1.02|     9.62|     9.07|        29.78|2017-12-08|Andhra Pradesh| 234.0|  30.88|\n",
      "|           2.19|          2.65|    0.75|     11.3|     13.9|        29.23|2017-12-09|Andhra Pradesh|263.25|  27.92|\n",
      "|           2.19|          2.65|    1.51|     15.1|     7.75|        29.08|2017-12-24|Andhra Pradesh|  97.0|  12.78|\n",
      "|           2.19|          2.65|    0.57|    11.27|     7.88|        26.68|2018-01-05|Andhra Pradesh| 159.0|  12.78|\n",
      "|           2.19|          2.65|    0.57|    11.58|     3.52|        29.75|2018-02-01|Andhra Pradesh| 231.0|  12.78|\n",
      "|           2.19|          2.65|    0.57|     16.0|     5.95|        30.43|2018-02-03|Andhra Pradesh| 170.0|  12.78|\n",
      "|           2.19|          2.65|    0.57|    15.77|    12.57|        28.45|2018-02-06|Andhra Pradesh|181.25|  12.78|\n",
      "|           2.19|          2.65|    0.37|     2.55|     8.55|        29.97|2018-02-16|Andhra Pradesh| 273.0|   1.85|\n",
      "|           2.19|          2.65|    1.05|    12.72|    19.83|         29.8|2018-02-28|Andhra Pradesh| 242.0|  12.78|\n",
      "|           2.19|          2.65|    0.56|     20.5|     2.45|        33.27|2018-03-01|Andhra Pradesh|198.75|   14.2|\n",
      "|           2.19|          2.65|    0.57|    10.38|     7.75|         28.7|2018-03-04|Andhra Pradesh| 296.5|  49.45|\n",
      "|           2.19|          2.65|     1.1|    20.75|     7.75|        32.45|2018-03-16|Andhra Pradesh|215.25|  25.75|\n",
      "|           2.19|          2.65|    0.62|     5.87|      2.7|        29.97|2018-04-03|Andhra Pradesh|279.67|  18.37|\n",
      "|           2.19|          2.65|    0.99|     5.88|     6.48|        28.15|2018-04-09|Andhra Pradesh|167.25|  12.78|\n",
      "|           2.19|          2.65|     0.6|     12.2|     5.02|        28.42|2018-04-21|Andhra Pradesh| 315.0|  21.62|\n",
      "|           2.19|          2.65|    0.25|     5.97|    14.15|        28.53|2018-05-06|Andhra Pradesh| 271.0|   32.6|\n",
      "|           2.19|          2.65|    0.12|     7.85|     7.75|        28.47|2018-06-08|Andhra Pradesh| 289.5|  34.95|\n",
      "|           2.19|          2.65|    0.27|     7.77|     6.23|        27.65|2018-06-11|Andhra Pradesh|280.25|  25.75|\n",
      "|           2.19|          2.65|    0.46|    11.38|     7.75|         29.0|2018-06-24|Andhra Pradesh|306.75|  22.23|\n",
      "|           2.19|          2.65|    0.58|      6.5|     0.53|        27.43|2018-07-21|Andhra Pradesh|272.67|  35.93|\n",
      "|           2.19|          2.65|    0.72|    14.07|      3.1|         30.5|2018-07-28|Andhra Pradesh| 277.0|   23.5|\n",
      "|           2.19|          2.65|    0.57|      4.8|      9.8|        26.65|2018-08-31|Andhra Pradesh|  24.5|  12.78|\n",
      "|           2.19|          2.65|    0.32|    10.02|     7.62|        26.25|2018-10-01|Andhra Pradesh| 140.0|   12.2|\n",
      "|           2.19|          2.65|    0.38|     15.8|      2.3|         30.7|2018-10-09|Andhra Pradesh| 240.0|    8.3|\n",
      "|           2.19|          2.65|    0.75|    14.83|     6.35|        29.05|2018-10-12|Andhra Pradesh|201.75|   25.1|\n",
      "|           2.19|          2.65|    0.68|      6.2|     2.15|        29.55|2018-11-04|Andhra Pradesh|129.25|    8.4|\n",
      "+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet and Show Columns\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/final_parquet/*.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Get the list of column names\n",
    "columns = df.columns\n",
    "\n",
    "# Limit to the first 10 columns (if there are that many)\n",
    "limited_columns = columns[2:12]\n",
    "\n",
    "# Select only the limited columns from the DataFrame\n",
    "df_limited = df.select(*limited_columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_limited.show(50)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f478e85",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf75027",
   "metadata": {},
   "source": [
    "### first read schema from file for hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30a1bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the Parquet file:\n",
      "StructType(List(StructField(bp_mmhg,DoubleType,true),StructField(benzene_ug_m3,DoubleType,true),StructField(mp_xylene_ug_m3,DoubleType,true),StructField(o_xylene_ug_m3,DoubleType,true),StructField(co_mg_m3,DoubleType,true),StructField(nh3_ug_m3,DoubleType,true),StructField(so2_ug_m3,DoubleType,true),StructField(temp_degree_c,DoubleType,true),StructField(from_date,DateType,true),StructField(state,StringType,true),StructField(wd_deg,DoubleType,true),StructField(nox_ppb,DoubleType,true),StructField(no_ug_m3,DoubleType,true),StructField(toluene_ug_m3,DoubleType,true),StructField(to_date,DateType,true),StructField(ozone_ug_m3,DoubleType,true),StructField(pm10_ug_m3,DoubleType,true),StructField(sr_w_mt2,DoubleType,true),StructField(ws_m_s,DoubleType,true),StructField(no2_ug_m3,DoubleType,true),StructField(pm2_5_ug_m3,DoubleType,true),StructField(rh_%,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet and Extract Schema\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/final_parquet/*.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Extract and print the schema\n",
    "schema = df.schema\n",
    "print(\"Schema of the Parquet file:\")\n",
    "print(schema)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae11ce",
   "metadata": {},
   "source": [
    "### create table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE AQI (\n",
    "#     bp_mmhg DOUBLE,\n",
    "#     benzene_ug_m3 DOUBLE,\n",
    "#     mp_xylene_ug_m3 DOUBLE,\n",
    "#     o_xylene_ug_m3 DOUBLE,\n",
    "#     co_mg_m3 DOUBLE,\n",
    "#     nh3_ug_m3 DOUBLE,\n",
    "#     so2_ug_m3 DOUBLE,\n",
    "#     temp_degree_c DOUBLE,\n",
    "#     from_date TIMESTAMP,\n",
    "#     state STRING,\n",
    "#     wd_deg DOUBLE,\n",
    "#     nox_ppb DOUBLE,\n",
    "#     no_ug_m3 DOUBLE,\n",
    "#     toluene_ug_m3 DOUBLE,\n",
    "#     to_date TIMESTAMP,\n",
    "#     ozone_ug_m3 DOUBLE,\n",
    "#     pm10_ug_m3 DOUBLE,\n",
    "#     sr_w_mt2 DOUBLE,\n",
    "#     ws_m_s DOUBLE,\n",
    "#     no2_ug_m3 DOUBLE,\n",
    "#     pm2_5_ug_m3 DOUBLE,\n",
    "#     rh_ DOUBLE\n",
    "# )\n",
    "# STORED AS PARQUET;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb59d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|        bp_mmhg|   double|   null|\n",
      "|  benzene_ug_m3|   double|   null|\n",
      "|mp_xylene_ug_m3|   double|   null|\n",
      "| o_xylene_ug_m3|   double|   null|\n",
      "|       co_mg_m3|   double|   null|\n",
      "|      nh3_ug_m3|   double|   null|\n",
      "|      so2_ug_m3|   double|   null|\n",
      "|  temp_degree_c|   double|   null|\n",
      "|      from_date|timestamp|   null|\n",
      "|          state|   string|   null|\n",
      "|         wd_deg|   double|   null|\n",
      "|        nox_ppb|   double|   null|\n",
      "|       no_ug_m3|   double|   null|\n",
      "|  toluene_ug_m3|   double|   null|\n",
      "|        to_date|timestamp|   null|\n",
      "|    ozone_ug_m3|   double|   null|\n",
      "|     pm10_ug_m3|   double|   null|\n",
      "|       sr_w_mt2|   double|   null|\n",
      "|         ws_m_s|   double|   null|\n",
      "|      no2_ug_m3|   double|   null|\n",
      "+---------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+--------+-------------+----------+-----------+----------+--------+------+---------+-----------+-----+\n",
      "|bp_mmhg|benzene_ug_m3|mp_xylene_ug_m3|o_xylene_ug_m3|co_mg_m3|nh3_ug_m3|so2_ug_m3|temp_degree_c| from_date|         state|wd_deg|nox_ppb|no_ug_m3|toluene_ug_m3|   to_date|ozone_ug_m3|pm10_ug_m3|sr_w_mt2|ws_m_s|no2_ug_m3|pm2_5_ug_m3| rh_%|\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+--------+-------------+----------+-----------+----------+--------+------+---------+-----------+-----+\n",
      "| 740.75|         2.65|           2.19|          2.65|    0.87|      3.8|     20.2|         27.0|2017-07-03|Andhra Pradesh|297.75|  12.13|    3.68|         8.53|2017-07-03|       7.72|      51.5|     3.0|  1.62|    17.15|      23.75|77.75|\n",
      "|  734.5|         0.53|           2.19|          2.65|     0.0|     3.65|     20.2|        27.45|2017-07-07|Andhra Pradesh|288.75|  13.12|     2.5|         1.75|2017-07-07|      15.62|     157.5|     3.0|  1.75|    20.93|       75.5| 78.0|\n",
      "| 736.75|         0.53|           2.19|          2.65|     0.0|      3.2|     6.85|         27.0|2017-07-08|Andhra Pradesh| 288.5|   8.72|    1.58|         1.75|2017-07-08|        3.9|     121.5|     3.0|  1.65|     14.0|       67.5|74.75|\n",
      "|  733.0|         2.65|           2.19|          2.65|     0.0|     3.48|     2.62|        29.63|2017-07-15|Andhra Pradesh| 295.5|    6.0|     1.5|         1.75|2017-07-15|      13.88|      56.0|     3.0|   2.7|      9.0|       23.5| 74.5|\n",
      "|  730.0|         0.53|           2.19|          2.65|     0.0|     2.38|     7.75|        44.43|2017-07-17|Andhra Pradesh| 299.5|   8.63|    1.97|        10.75|2017-07-17|      43.78|      40.5|     6.5|   2.4|    13.12|      33.75| 87.0|\n",
      "|  737.0|         2.73|           2.19|          2.65|    0.01|     4.13|    18.03|        27.83|2017-07-24|Andhra Pradesh| 286.0|    5.6|    1.43|         7.53|2017-07-24|       15.5|     112.0|     3.0|  3.27|     8.33|       32.0|66.33|\n",
      "| 733.25|         0.53|           2.19|          2.65|     0.0|      2.5|    13.95|        27.87|2017-08-16|Andhra Pradesh| 307.5|   9.65|    1.45|         1.75|2017-08-16|       49.2|    114.25|     3.0|  2.45|    15.95|      58.75| 67.5|\n",
      "|  736.0|         0.53|           2.19|          2.65|     0.0|     6.93|    20.92|        36.13|2017-08-20|Andhra Pradesh| 299.5|  11.48|     1.4|         9.45|2017-08-20|       10.5|      99.0|     3.5|   0.3|    19.48|       32.0|78.75|\n",
      "|  732.5|         1.95|           2.19|          2.65|     0.0|     2.72|     15.1|        28.47|2017-08-25|Andhra Pradesh|199.75|   4.77|    1.88|         5.43|2017-08-25|      23.93|      81.5|     3.0|  1.05|      6.1|      39.25|71.75|\n",
      "| 729.75|          2.5|           2.19|          2.65|     0.0|    10.02|     7.75|         55.6|2017-08-27|Andhra Pradesh| 249.5|  11.72|    2.55|         0.53|2017-08-27|      23.93|     94.25|     5.0|  0.62|    18.18|       40.5| 80.5|\n",
      "|  742.0|         0.53|           2.19|          2.65|     0.0|      1.9|     2.73|        27.77|2017-09-01|Andhra Pradesh|  81.0|    8.1|    2.97|         1.75|2017-09-02|        8.5|     61.25|    4.33|   0.3|     10.7|       81.0|82.33|\n",
      "| 747.67|          2.9|           2.19|          2.65|     0.0|     1.17|     5.27|        28.93|2017-09-11|Andhra Pradesh| 191.0|   3.87|    1.17|          8.5|2017-09-11|       8.53|      43.0|     3.0|   0.3|     5.47|       19.0| 79.0|\n",
      "| 737.75|         2.27|           2.19|          2.65|     0.0|      3.3|     7.75|        28.48|2017-09-15|Andhra Pradesh| 249.5|   5.08|    2.02|        10.65|2017-09-15|      16.43|     45.25|     3.0|  2.62|     6.45|       20.0|67.75|\n",
      "|  218.5|          1.9|           2.19|          2.65|     0.0|     3.32|     5.32|        27.92|2017-09-23|Andhra Pradesh|  86.0|   6.28|    1.43|         5.95|2017-09-23|       5.88|     102.5|     5.0|   0.3|      9.8|      38.75| 83.5|\n",
      "|  218.5|         0.53|           2.19|          2.65|     0.0|      1.9|     8.75|         29.0|2017-09-25|Andhra Pradesh| 275.0|    2.3|    4.05|          9.7|2017-09-25|       3.85|     122.5|     3.0|  0.43|    17.05|       68.5| 81.0|\n",
      "|  218.5|         2.93|           2.19|          2.65|     0.0|     0.17|     0.63|         28.2|2017-09-27|Andhra Pradesh|  30.0|   0.63|    3.67|          9.1|2017-09-28|      23.93|      63.0|     5.0|   0.3|    17.05|       30.0| 76.0|\n",
      "|  741.0|          3.1|           2.19|          2.65|     0.9|     0.23|     0.63|        27.83|2017-09-28|Andhra Pradesh|  45.0|    0.3|    2.07|          9.3|2017-09-28|      23.93|      40.0|     4.0|  1.27|    17.05|       15.0| 80.0|\n",
      "|  218.5|         0.53|           2.19|          2.65|    1.14|     0.08|     7.75|        32.32|2017-09-30|Andhra Pradesh| 171.0|   0.92|    4.15|         1.75|2017-09-30|      23.93|      74.0|     3.0|   0.5|    17.05|      29.75| 77.0|\n",
      "|  218.5|         0.53|           2.19|          2.65|    0.57|     0.43|    16.77|         31.9|2017-09-30|Andhra Pradesh| 190.0|    0.7|    3.27|         1.75|2017-10-01|      23.93|     144.0|     5.0|   0.3|    17.05|       48.0| 82.0|\n",
      "|  218.5|          0.0|           2.19|          2.65|     0.0|     0.03|     7.75|        28.62|2017-10-14|Andhra Pradesh|  86.0|    0.0|    2.92|          0.0|2017-10-14|      23.93|     61.25|     0.0|  2.07|    17.05|       27.0| 41.0|\n",
      "|  218.5|         0.53|           2.19|          2.65|    0.88|     0.13|    19.87|        17.57|2017-10-19|Andhra Pradesh|101.33|   0.03|    1.33|         8.33|2017-10-20|       1.83|     61.25|    1.33|  0.23|    17.05|       27.0|27.33|\n",
      "|  218.5|          1.8|           2.19|          2.65|     0.5|    10.02|     4.92|         21.3|2017-11-04|Andhra Pradesh|  92.0|   0.45|    0.93|          6.5|2017-11-05|      45.45|      42.0|     3.0|  1.55|    17.05|      14.25|56.25|\n",
      "|  748.0|          1.7|           2.19|          2.65|     0.7|      0.4|     4.13|        28.97|2017-11-13|Andhra Pradesh|155.33|   4.87|    0.87|         1.63|2017-11-14|       92.2|      59.0|     4.0|   2.4|     8.17|       30.0|66.33|\n",
      "|  746.5|         1.92|           2.19|          2.65|    0.63|     0.92|     4.73|        29.73|2017-11-18|Andhra Pradesh|  53.0|   9.32|    3.48|         1.75|2017-11-18|      11.53|      15.5|    4.75|   0.5|    12.23|       5.75| 83.0|\n",
      "| 749.75|         2.95|           2.19|          2.65|    1.02|     9.62|     9.07|        29.78|2017-12-08|Andhra Pradesh| 234.0|  30.88|    4.02|         5.65|2017-12-08|      47.67|     83.75|     4.0|   1.1|    51.85|      44.75| 46.5|\n",
      "|  746.5|         1.95|           2.19|          2.65|    0.75|     11.3|     13.9|        29.23|2017-12-09|Andhra Pradesh|263.25|  27.92|    1.45|         2.93|2017-12-09|       49.4|      97.0|    4.25|  1.32|    50.25|      45.75| 57.5|\n",
      "|  754.5|         0.53|           2.19|          2.65|    1.51|     15.1|     7.75|        29.08|2017-12-24|Andhra Pradesh|  97.0|  12.78|    2.92|         1.75|2017-12-24|       3.35|     61.25|     5.0|   0.3|    17.05|       27.0|84.75|\n",
      "|  753.0|         0.53|           2.19|          2.65|    0.57|    11.27|     7.88|        26.68|2018-01-05|Andhra Pradesh| 159.0|  12.78|    2.92|         1.75|2018-01-05|       7.65|     177.5|     4.0|   0.3|    17.05|       87.0|66.75|\n",
      "| 753.25|         0.53|           2.19|          2.65|    0.57|    11.58|     3.52|        29.75|2018-02-01|Andhra Pradesh| 231.0|  12.78|    2.92|         1.75|2018-02-01|       28.2|     127.0|    5.75|   0.7|    17.05|      52.25| 58.5|\n",
      "| 755.75|         0.53|           2.19|          2.65|    0.57|     16.0|     5.95|        30.43|2018-02-03|Andhra Pradesh| 170.0|  12.78|    2.92|         1.75|2018-02-03|      16.85|     117.0|     6.0|   0.6|    17.05|      46.75|62.25|\n",
      "|  762.0|         0.53|           2.19|          2.65|    0.57|    15.77|    12.57|        28.45|2018-02-06|Andhra Pradesh|181.25|  12.78|    2.92|         1.75|2018-02-06|        4.8|     61.25|     3.0|   0.3|    17.05|       27.0| 65.5|\n",
      "| 762.25|         3.43|           2.19|          2.65|    0.37|     2.55|     8.55|        29.97|2018-02-16|Andhra Pradesh| 273.0|   1.85|    0.65|         5.23|2018-02-16|      88.27|      97.5|     3.0|   2.3|     2.45|      46.25| 64.5|\n",
      "| 755.25|         0.53|           2.19|          2.65|    1.05|    12.72|    19.83|         29.8|2018-02-28|Andhra Pradesh| 242.0|  12.78|    2.92|         1.75|2018-02-28|      17.35|     61.25|     3.0|  1.32|    17.05|       88.5|44.75|\n",
      "|  750.5|         0.53|           2.19|          2.65|    0.56|     20.5|     2.45|        33.27|2018-03-01|Andhra Pradesh|198.75|   14.2|     1.7|         1.75|2018-03-01|      83.73|    100.75|     3.0|  3.68|     24.0|      45.25| 62.0|\n",
      "|  747.5|         0.53|           2.19|          2.65|    0.57|    10.38|     7.75|         28.7|2018-03-04|Andhra Pradesh| 296.5|  49.45|   11.23|         9.65|2018-03-04|      22.75|     61.25|     6.0|  1.58|    17.05|      49.75| 56.5|\n",
      "| 752.75|         0.53|           2.19|          2.65|     1.1|    20.75|     7.75|        32.45|2018-03-16|Andhra Pradesh|215.25|  25.75|    7.38|         1.75|2018-03-16|      23.22|    119.75|     3.0|  1.97|    37.17|      54.25|67.25|\n",
      "| 746.33|         0.53|           2.19|          2.65|    0.62|     5.87|      2.7|        29.97|2018-04-03|Andhra Pradesh|279.67|  18.37|    5.57|          7.3|2018-04-03|      16.93|     58.33|     3.0|  3.23|    26.03|      21.67| 75.0|\n",
      "|  750.5|         0.53|           2.19|          2.65|    0.99|     5.88|     6.48|        28.15|2018-04-09|Andhra Pradesh|167.25|  12.78|    2.92|         1.75|2018-04-09|       4.17|     183.5|     3.0|   0.5|    17.05|      69.75| 81.0|\n",
      "| 747.75|         3.15|           2.19|          2.65|     0.6|     12.2|     5.02|        28.42|2018-04-21|Andhra Pradesh| 315.0|  21.62|   10.23|          6.5|2018-04-21|       9.95|      71.5|     3.0|  3.62|    25.05|       21.0| 78.0|\n",
      "|  748.5|         2.65|           2.19|          2.65|    0.25|     5.97|    14.15|        28.53|2018-05-06|Andhra Pradesh| 271.0|   32.6|    2.92|         1.75|2018-05-06|        1.1|     62.25|    9.75|  2.25|    27.95|      17.75| 82.0|\n",
      "|  733.0|         0.53|           2.19|          2.65|    0.12|     7.85|     7.75|        28.47|2018-06-08|Andhra Pradesh| 289.5|  34.95|     7.4|          8.5|2018-06-08|       15.6|      75.0|     6.0|  3.98|    54.45|       24.0| 75.0|\n",
      "|  729.5|         2.55|           2.19|          2.65|    0.27|     7.77|     6.23|        27.65|2018-06-11|Andhra Pradesh|280.25|  25.75|    5.72|         6.55|2018-06-11|      22.72|     90.75|    8.25|  2.52|    39.68|      46.75|75.75|\n",
      "|  737.0|          2.7|           2.19|          2.65|    0.46|    11.38|     7.75|         29.0|2018-06-24|Andhra Pradesh|306.75|  22.23|    1.92|          5.2|2018-06-24|      18.27|     83.75|     5.0|   2.7|     38.9|      30.25| 78.0|\n",
      "|  736.0|         0.53|           2.19|          2.65|    0.58|      6.5|     0.53|        27.43|2018-07-21|Andhra Pradesh|272.67|  35.93|    21.1|         1.75|2018-07-21|       8.13|      39.0|     3.0|   2.4|    35.23|      26.33| 87.0|\n",
      "| 740.67|         0.53|           2.19|          2.65|    0.72|    14.07|      3.1|         30.5|2018-07-28|Andhra Pradesh| 277.0|   23.5|    11.7|        10.77|2018-07-28|       17.6|      60.0|     3.0|  2.07|    26.27|       23.0| 72.0|\n",
      "|  741.0|         0.53|           2.19|          2.65|    0.57|      4.8|      9.8|        26.65|2018-08-31|Andhra Pradesh|  24.5|  12.78|    2.92|         1.75|2018-08-31|       7.95|      92.0|     6.0|  0.35|    17.05|       41.0| 77.0|\n",
      "|  745.5|         3.35|           2.19|          2.65|    0.32|    10.02|     7.62|        26.25|2018-10-01|Andhra Pradesh| 140.0|   12.2|     3.4|          7.4|2018-10-01|      26.95|     91.25|    16.5|  1.93|    17.75|       63.0| 73.0|\n",
      "|  745.0|          1.9|           2.19|          2.65|    0.38|     15.8|      2.3|         30.7|2018-10-09|Andhra Pradesh| 240.0|    8.3|     2.8|          2.5|2018-10-09|       23.7|     103.0|     6.0|   4.0|     11.4|       41.0| 66.0|\n",
      "| 748.75|         0.53|           2.19|          2.65|    0.75|    14.83|     6.35|        29.05|2018-10-12|Andhra Pradesh|201.75|   25.1|     3.4|         1.75|2018-10-12|      39.42|     156.0|     5.5|  2.42|    41.95|      66.75| 73.5|\n",
      "| 756.25|         2.02|           2.19|          2.65|    0.68|      6.2|     2.15|        29.55|2018-11-04|Andhra Pradesh|129.25|    8.4|    2.98|          5.3|2018-11-04|      20.63|     27.25|     6.0|  1.57|     11.2|       0.75| 75.0|\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+------+-------+--------+-------------+----------+-----------+----------+--------+------+---------+-----------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Hive Integration\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Describe the Hive table\n",
    "spark.sql(\"DESCRIBE project.aqi\").show()\n",
    "\n",
    "# Read a Parquet file into a DataFrame\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/final_parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Write Data to the Hive table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"project.aqi\")\n",
    "\n",
    "# Read the data from the Hive table\n",
    "df_from_hive = spark.sql(\"SELECT * FROM project.aqi\")\n",
    "\n",
    "# Show the top 50 rows\n",
    "df_from_hive.show(50)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f0fd292",
   "metadata": {},
   "source": [
    "### Exporting data for tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dd90320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to file:///home/talentum/myproject/dataSource/output/tableau\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveToCSV\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the Hive table name\n",
    "hive_table = \"project.aqi\"\n",
    "\n",
    "# Read data from Hive table\n",
    "df = spark.sql(f\"SELECT * FROM {hive_table}\")\n",
    "\n",
    "df = df.coalesce(1)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_file_path = \"file:///home/talentum/myproject/dataSource/output/tableau\"\n",
    "\n",
    "# Save the Spark DataFrame as a CSV file\n",
    "df.write.csv(output_file_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Data saved to {output_file_path}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995a750",
   "metadata": {},
   "source": [
    "# ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad3cf9",
   "metadata": {},
   "source": [
    "### Sample size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c81b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bp_mmhg: double (nullable = true)\n",
      " |-- benzene_ug_m3: double (nullable = true)\n",
      " |-- mp_xylene_ug_m3: double (nullable = true)\n",
      " |-- o_xylene_ug_m3: double (nullable = true)\n",
      " |-- co_mg_m3: double (nullable = true)\n",
      " |-- nh3_ug_m3: double (nullable = true)\n",
      " |-- so2_ug_m3: double (nullable = true)\n",
      " |-- temp_degree_c: double (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- wd_deg: double (nullable = true)\n",
      " |-- nox_ppb: double (nullable = true)\n",
      " |-- no_ug_m3: double (nullable = true)\n",
      " |-- toluene_ug_m3: double (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- ozone_ug_m3: double (nullable = true)\n",
      " |-- pm10_ug_m3: double (nullable = true)\n",
      " |-- sr_w_mt2: double (nullable = true)\n",
      " |-- ws_m_s: double (nullable = true)\n",
      " |-- no2_ug_m3: double (nullable = true)\n",
      " |-- pm2_5_ug_m3: double (nullable = true)\n",
      " |-- rh_%: double (nullable = true)\n",
      "\n",
      "DataFrame Dimensions: 10000 rows, 10 columns\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+\n",
      "|bp_mmhg|benzene_ug_m3|mp_xylene_ug_m3|o_xylene_ug_m3|co_mg_m3|nh3_ug_m3|so2_ug_m3|temp_degree_c| from_date|         state|\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+\n",
      "|  25.68|          0.6|           2.19|          2.65|    0.59|    17.35|    23.05|        76.75|2023-03-23|Madhya Pradesh|\n",
      "|  41.25|         0.63|           2.19|          2.65|   40.38|    15.83|    46.45|        27.95|2022-11-04|         Bihar|\n",
      "|  17.88|          0.0|           1.25|          29.0|    0.93|      5.5|      8.4|        27.95|2020-03-18| Uttar Pradesh|\n",
      "|   1.39|          1.4|           2.19|          2.65|    0.55|    15.83|     5.45|        27.95|2022-09-19|         Bihar|\n",
      "|    0.0|         1.05|           0.68|          0.97|    0.97|    87.23|      9.8|        100.0|2013-04-11|         Delhi|\n",
      "+-------+-------------+---------------+--------------+--------+---------+---------+-------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, to_date, col\n",
    "from pyspark.sql.types import StringType\n",
    "from math import ceil\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeDatasetProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark configurations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"300\")  # Increase if shuffle operations are involved\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")  # Use G1 garbage collector\n",
    "spark.conf.set(\"spark.memory.fraction\", \"0.6\")  # Memory fraction for caching and execution\n",
    "spark.conf.set(\"spark.network.timeout\", \"800s\")\n",
    "spark.conf.set(\"spark.rpc.askTimeout\", \"600s\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # Increase based on the number of available cores\n",
    "spark.conf.set(\"spark.shuffle.file.buffer\", \"128k\")  # Buffer size for shuffle files\n",
    "\n",
    "# Load data from a Parquet file (replace 'your_parquet_file_path' with the actual file path)\n",
    "df = spark.read.parquet(\"file:///home/talentum/myproject/dataSource/output/final_parquet/*.parquet\")\n",
    "\n",
    "# Show schema to verify the format of 'from_date' and 'to_date'\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# Step 1: Randomize the data\n",
    "randomized_df = df.orderBy(rand())\n",
    "\n",
    "# Step 2: Calculate suitable sample size\n",
    "population_size = df.count()  # Total rows in the dataset\n",
    "confidence_level = 1.96  # Z-score for 95% confidence\n",
    "margin_of_error = 0.01  # 1% margin of error\n",
    "std_dev = 0.5  # Standard deviation (assumed for binary outcome, can be adjusted)\n",
    "\n",
    "# Sample size formula: n = (Z^2 * p * (1-p)) / e^2\n",
    "sample_size = (confidence_level**2 * std_dev * (1 - std_dev)) / margin_of_error**2\n",
    "\n",
    "# Adjust sample size for finite population correction\n",
    "finite_sample_size = ceil(sample_size / (1 + (sample_size - 1) / population_size))\n",
    "\n",
    "# Step 3: Extract the sample from the randomized data\n",
    "sampled_df = randomized_df.limit(finite_sample_size)\n",
    "\n",
    "# Select the first 10 columns\n",
    "selected_columns = sampled_df.columns[:10]\n",
    "pdf = sampled_df.select(*selected_columns)\n",
    "\n",
    "# To cache\n",
    "# pdf = pdf.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "num_rows = pdf.count()\n",
    "num_columns = len(pdf.columns)\n",
    "\n",
    "# Show dimensions\n",
    "print(f\"DataFrame Dimensions: {num_rows} rows, {num_columns} columns\")\n",
    "\n",
    "# Show the first 5 rows of the sample with only 10 columns\n",
    "pdf.show(5)\n",
    "\n",
    "# Unpersist and stop Spark session\n",
    "# pdf.unpersist()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01aae0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+-----+------------+\n",
      "|ws_m_s|no2_ug_m3|pm2_5_ug_m3|rh_% |AQI_Category|\n",
      "+------+---------+-----------+-----+------------+\n",
      "|1.62  |17.15    |23.75      |77.75|Moderate    |\n",
      "|1.75  |20.93    |75.5       |78.0 |Unhealthy   |\n",
      "|1.65  |14.0     |67.5       |74.75|Unhealthy   |\n",
      "|2.7   |9.0      |23.5       |74.5 |Moderate    |\n",
      "|2.4   |13.12    |33.75      |87.0 |Moderate    |\n",
      "|3.27  |8.33     |32.0       |66.33|Unhealthy   |\n",
      "|2.45  |15.95    |58.75      |67.5 |Unhealthy   |\n",
      "|0.3   |19.48    |32.0       |78.75|Moderate    |\n",
      "|1.05  |6.1      |39.25      |71.75|Moderate    |\n",
      "|0.62  |18.18    |40.5       |80.5 |Moderate    |\n",
      "+------+---------+-----------+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+-------+\n",
      "|  AQI_Category|  count|\n",
      "+--------------+-------+\n",
      "|          Good|2046877|\n",
      "|     Hazardous| 915638|\n",
      "|      Moderate|4253218|\n",
      "|     Unhealthy|3653919|\n",
      "|Very Unhealthy|1146848|\n",
      "+--------------+-------+\n",
      "\n",
      "['bp_mmhg', 'benzene_ug_m3', 'mp_xylene_ug_m3', 'o_xylene_ug_m3', 'co_mg_m3', 'nh3_ug_m3', 'so2_ug_m3', 'temp_degree_c', 'from_date', 'state', 'wd_deg', 'nox_ppb', 'no_ug_m3', 'toluene_ug_m3', 'to_date', 'ozone_ug_m3', 'pm10_ug_m3', 'sr_w_mt2', 'ws_m_s', 'no2_ug_m3', 'pm2_5_ug_m3', 'rh_%', 'AQI_Category']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.appName(\"AQI Calculation\").getOrCreate()\n",
    "\n",
    "# Define the function to calculate AQI and categorize it\n",
    "def calculate_aqi(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn(\n",
    "        \"AQI_Category\",\n",
    "        F.when(\n",
    "            (F.col(\"pm2_5_ug_m3\") <= 30) & (F.col(\"pm10_ug_m3\") <= 50) & (F.col(\"so2_ug_m3\") <= 40) & (F.col(\"no2_ug_m3\") <= 40),\n",
    "            \"Good\"\n",
    "        ).when(\n",
    "            (F.col(\"pm2_5_ug_m3\") <= 60) & (F.col(\"pm10_ug_m3\") <= 100) & (F.col(\"so2_ug_m3\") <= 80) & (F.col(\"no2_ug_m3\") <= 80),\n",
    "            \"Moderate\"\n",
    "        ).when(\n",
    "            (F.col(\"pm2_5_ug_m3\") <= 90) & (F.col(\"pm10_ug_m3\") <= 250) & (F.col(\"so2_ug_m3\") <= 380) & (F.col(\"no2_ug_m3\") <= 180),\n",
    "            \"Unhealthy\"\n",
    "        ).when(\n",
    "            (F.col(\"pm2_5_ug_m3\") > 120) | (F.col(\"pm10_ug_m3\") > 350) | (F.col(\"so2_ug_m3\") > 800) & (F.col(\"no2_ug_m3\") <= 280),\n",
    "            \"Very Unhealthy\"\n",
    "        ).otherwise(\"Hazardous\")\n",
    "    )\n",
    "\n",
    "# Load your DataFrame (replace 'path_to_parquet_file' with your actual file path)\n",
    "pdf = spark.read.parquet(\"file:///home/talentum/myproject/dataSource/output/final_parquet/*.parquet\")\n",
    "\n",
    "# Apply the AQI calculation function to the DataFrame\n",
    "df_with_aqi = calculate_aqi(pdf)\n",
    "\n",
    "# Select and show the last 5 columns of the DataFrame\n",
    "columns = df_with_aqi.columns[-5:]  # Get the last 5 column names\n",
    "df_with_aqi.select(*columns).show(10, truncate=False)  # Show the last 5 columns for the first 10 rows\n",
    "\n",
    "# Group by AQI_Category and count the occurrences\n",
    "df_with_aqi.groupBy(\"AQI_Category\").count().orderBy(\"AQI_Category\").show()\n",
    "\n",
    "# Print all the column names\n",
    "print(df_with_aqi.columns)\n",
    "\n",
    "# Save the DataFrame as a single Parquet file\n",
    "output_path = \"file:///home/talentum/myproject/dataSource/output/df_with_aqi_parquet\"\n",
    "\n",
    "# Coalesce the DataFrame to a single partition before writing\n",
    "df_with_aqi.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "\n",
    "# Stop the Spark session after use\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a76383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdb7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ccef56",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e209344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                                     # Popular plotting library\n",
    "#  matplotlib inline                                                  # To plot graphs inline the notebook\n",
    "import seaborn as sns                                               # Advanced plotting library\n",
    "# from handyspark import *    \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b03c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "bp_mmhg: 0.0034\n",
      "benzene_ug_m3: 0.0070\n",
      "mp_xylene_ug_m3: 0.0079\n",
      "o_xylene_ug_m3: 0.0169\n",
      "co_mg_m3: 0.0085\n",
      "nh3_ug_m3: 0.0275\n",
      "so2_ug_m3: 0.0006\n",
      "temp_degree_c: 0.0113\n",
      "wd_deg: 0.0017\n",
      "nox_ppb: 0.0021\n",
      "no_ug_m3: 0.0202\n",
      "toluene_ug_m3: 0.0117\n",
      "ozone_ug_m3: 0.0004\n",
      "pm10_ug_m3: 0.4527\n",
      "sr_w_mt2: 0.0018\n",
      "ws_m_s: 0.0034\n",
      "no2_ug_m3: 0.0221\n",
      "pm2_5_ug_m3: 0.4001\n",
      "rh_%: 0.0003\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+----------+---------+-----------+----------+----------+--------------+------------+\n",
      "|o_xylene_ug_m3|nh3_ug_m3|no_ug_m3|pm10_ug_m3|no2_ug_m3|pm2_5_ug_m3| from_date|   to_date|         state|AQI_Category|\n",
      "+--------------+---------+--------+----------+---------+-----------+----------+----------+--------------+------------+\n",
      "|          2.65|      3.8|    3.68|      51.5|    17.15|      23.75|2017-07-03|2017-07-03|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     3.65|     2.5|     157.5|    20.93|       75.5|2017-07-07|2017-07-07|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|      3.2|    1.58|     121.5|     14.0|       67.5|2017-07-08|2017-07-08|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|     3.48|     1.5|      56.0|      9.0|       23.5|2017-07-15|2017-07-15|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     2.38|    1.97|      40.5|    13.12|      33.75|2017-07-17|2017-07-17|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     4.13|    1.43|     112.0|     8.33|       32.0|2017-07-24|2017-07-24|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|      2.5|    1.45|    114.25|    15.95|      58.75|2017-08-16|2017-08-16|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|     6.93|     1.4|      99.0|    19.48|       32.0|2017-08-20|2017-08-20|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     2.72|    1.88|      81.5|      6.1|      39.25|2017-08-25|2017-08-25|Andhra Pradesh|    Moderate|\n",
      "|          2.65|    10.02|    2.55|     94.25|    18.18|       40.5|2017-08-27|2017-08-27|Andhra Pradesh|    Moderate|\n",
      "|          2.65|      1.9|    2.97|     61.25|     10.7|       81.0|2017-09-01|2017-09-02|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|     1.17|    1.17|      43.0|     5.47|       19.0|2017-09-11|2017-09-11|Andhra Pradesh|        Good|\n",
      "|          2.65|      3.3|    2.02|     45.25|     6.45|       20.0|2017-09-15|2017-09-15|Andhra Pradesh|        Good|\n",
      "|          2.65|     3.32|    1.43|     102.5|      9.8|      38.75|2017-09-23|2017-09-23|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|      1.9|    4.05|     122.5|    17.05|       68.5|2017-09-25|2017-09-25|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|     0.17|    3.67|      63.0|    17.05|       30.0|2017-09-27|2017-09-28|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     0.23|    2.07|      40.0|    17.05|       15.0|2017-09-28|2017-09-28|Andhra Pradesh|        Good|\n",
      "|          2.65|     0.08|    4.15|      74.0|    17.05|      29.75|2017-09-30|2017-09-30|Andhra Pradesh|    Moderate|\n",
      "|          2.65|     0.43|    3.27|     144.0|    17.05|       48.0|2017-09-30|2017-10-01|Andhra Pradesh|   Unhealthy|\n",
      "|          2.65|     0.03|    2.92|     61.25|    17.05|       27.0|2017-10-14|2017-10-14|Andhra Pradesh|    Moderate|\n",
      "+--------------+---------+--------+----------+---------+-----------+----------+----------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "12016500\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AQI Feature Importance\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read DataFrame from Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/df_with_aqi_parquet/*.parquet\"\n",
    "df_with_aqi = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Encode the target column 'AQI_Category' if it's categorical\n",
    "indexer = StringIndexer(inputCol='AQI_Category', outputCol='label')\n",
    "\n",
    "# Define the features (excluding the target and any irrelevant columns)\n",
    "feature_columns = ['bp_mmhg', 'benzene_ug_m3', 'mp_xylene_ug_m3', 'o_xylene_ug_m3', 'co_mg_m3',\n",
    "                   'nh3_ug_m3', 'so2_ug_m3', 'temp_degree_c', 'wd_deg', 'nox_ppb', \n",
    "                   'no_ug_m3', 'toluene_ug_m3', 'ozone_ug_m3', 'pm10_ug_m3', 'sr_w_mt2',\n",
    "                   'ws_m_s', 'no2_ug_m3', 'pm2_5_ug_m3', 'rh_%']\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a pipeline to apply StringIndexer and VectorAssembler\n",
    "pipeline = Pipeline(stages=[indexer, assembler])\n",
    "\n",
    "# Fit and transform the data\n",
    "df_prepared = pipeline.fit(df_with_aqi).transform(df_with_aqi)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(df_prepared)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# Create a list of (feature, importance) tuples\n",
    "feature_importance_list = [(feature, importance) for feature, importance in zip(feature_columns, importances)]\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in feature_importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Convert the feature importance list to a Pandas DataFrame for easier plotting\n",
    "importance_df = pd.DataFrame(feature_importance_list, columns=['Feature', 'Importance'])\n",
    "\n",
    "# Sort the DataFrame by importance for better visualization\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important features on top\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Filter based on a threshold (e.g., importance >= 0.0520)\n",
    "selected_features = [feature for feature, importance in feature_importance_list if importance >0.012446]\n",
    "\n",
    "# Add additional columns like 'from_date', 'to_date', 'state'\n",
    "selected_features.extend(['from_date', 'to_date', 'state',\"AQI_Category\"])\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "new_df = df_with_aqi.select(*selected_features)\n",
    "\n",
    "# Show the new DataFrame with selected features\n",
    "new_df.show()\n",
    "print(new_df.count())\n",
    "\n",
    "# Save the DataFrame as a single Parquet file\n",
    "output_path = \"file:///home/talentum/myproject/dataSource/output/df_for_ML\"\n",
    "\n",
    "# Coalesce the DataFrame to a single partition before writing\n",
    "df_with_aqi.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b72ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47768e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef878b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe55cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8daf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install plotly\n",
    "# pdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be131f",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df38f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Import the machine learning models\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5c7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Prediction Model\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # Read DataFrame from Parquet file\n",
    "parquet_file_path = \"file:///home/talentum/myproject/dataSource/output/df_for_ML/*.parquet\"\n",
    "df_with_aqi = spark.read.parquet(parquet_file_path)\n",
    "# new_df=df_with_aqi\n",
    "\n",
    "\n",
    "# Get a random sample of rows and limit to 100,000 rows\n",
    "new_df = df_with_aqi.sample(withReplacement=False, fraction=1.0, seed=42).limit(1000000)\n",
    "\n",
    "# Print the count of rows in the new DataFrame\n",
    "print(new_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680faa1",
   "metadata": {},
   "source": [
    "### 1) First, let's prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febd20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer for the categorical target column\n",
    "indexer = StringIndexer(inputCol='AQI_Category', outputCol='label')\n",
    "\n",
    "# VectorAssembler to combine feature columns into a single vector\n",
    "feature_cols = [col for col in new_df.columns if col not in ['AQI_Category', 'from_date', 'to_date', 'state']]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = new_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c331ac",
   "metadata": {},
   "source": [
    "### 2) Model Building with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b666aab",
   "metadata": {},
   "source": [
    "#### Now, let's define the models with hyperparameter grids and use cross-validation for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28247a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grids\n",
    "lr_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [0.1, 0.01])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                 .build())\n",
    "\n",
    "dt_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(dt.maxDepth, [5, 10, 20])\n",
    "                 .addGrid(dt.maxBins, [32, 64])\n",
    "                 .build())\n",
    "\n",
    "rf_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.numTrees, [20, 50])\n",
    "                 .addGrid(rf.maxDepth, [5, 10])\n",
    "                 .build())\n",
    "\n",
    "# Define CrossValidator\n",
    "crossval = CrossValidator(estimator=None,\n",
    "                          estimatorParamMaps=None,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy'),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Set up pipelines and cross-validate each model\n",
    "def cross_validate_model(model, param_grid, train_df):\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, model])\n",
    "    crossval.setEstimator(pipeline)\n",
    "    crossval.setEstimatorParamMaps(param_grid)\n",
    "    cv_model = crossval.fit(train_df)\n",
    "    return cv_model\n",
    "\n",
    "# Train and tune models\n",
    "lr_model = cross_validate_model(lr, lr_param_grid, train_df)\n",
    "dt_model = cross_validate_model(dt, dt_param_grid, train_df)\n",
    "rf_model = cross_validate_model(rf, rf_param_grid, train_df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e808db",
   "metadata": {},
   "source": [
    "### 3) Model Evaluation\n",
    "\n",
    "After tuning the models, let's evaluate their performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest']\n",
    "predictions = [lr_model.transform(test_df), \n",
    "               dt_model.transform(test_df), \n",
    "               rf_model.transform(test_df)]\n",
    "\n",
    "# Initialize evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "rmse_evaluator = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "accuracies = []\n",
    "rmses = []\n",
    "\n",
    "for preds in predictions:\n",
    "    accuracy = accuracy_evaluator.evaluate(preds)\n",
    "    # For RMSE, RegressionEvaluator expects continuous numerical predictions,\n",
    "    # which is not typical for classification, but you can still compute it as:\n",
    "    rmse = rmse_evaluator.evaluate(preds)\n",
    "    accuracies.append(accuracy)\n",
    "    rmses.append(rmse)\n",
    "\n",
    "# Create a summary DataFrame for comparison\n",
    "summary_df = spark.createDataFrame(zip(models, accuracies, rmses), schema=['Model', 'Accuracy', 'RMSE'])\n",
    "\n",
    "# Display the summary\n",
    "summary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8145fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 1 lakh rows\n",
    "# +-------------------+------------------+-------------------+\n",
    "# |              Model|          Accuracy|               RMSE|\n",
    "# +-------------------+------------------+-------------------+\n",
    "# |Logistic Regression|0.7868275515334339| 0.7197214700629562|\n",
    "# |      Decision Tree|0.9862745098039216|0.18786728732554484|\n",
    "# |      Random Forest|0.9850678733031675|  0.184219031545903|\n",
    "# +-------------------+------------------+-------------------+\n",
    "\n",
    "# for 5 lakh rows\n",
    "# +-------------------+------------------+------------------+\n",
    "# |              Model|          Accuracy|              RMSE|\n",
    "# +-------------------+------------------+------------------+\n",
    "# |Logistic Regression|0.7566911440150876|1.1789581067226291|\n",
    "# |      Decision Tree| 0.980036916656635|0.3303404826492617|\n",
    "# |      Random Forest|0.9631335018658962| 0.452766819705201|\n",
    "# +-------------------+------------------+------------------+\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f036218",
   "metadata": {},
   "source": [
    "### 4) Visualization\n",
    "\n",
    "Finally, we will plot the accuracies and RMSE values for comparison.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# Collect the summary data to plot\n",
    "summary_pd = summary_df.toPandas()\n",
    "\n",
    "# Plot accuracy and RMSE\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(summary_pd['Model'], summary_pd['Accuracy'], color='blue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(summary_pd['Model'], summary_pd['RMSE'], color='green')\n",
    "plt.xlabel('RMSE')\n",
    "plt.title('Model RMSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4375467",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = new_df\n",
    "pdf = pdf.orderBy(col(\"from_date\"))\n",
    "# Convert to Pandas DataFrame for ARIMA modeling\n",
    "pandas_df = pdf.toPandas()\n",
    "\n",
    "# Set the date as the index\n",
    "pandas_df.set_index('from_date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler  \n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "\n",
    "# Split data into train and test\n",
    "train, test = model_selection.train_test_split(pandas_df, test_size=0.2)\n",
    "\n",
    "# Fit the ARIMA model\n",
    "arima_model = pm.auto_arima(train['pm10_ug_m3'], \n",
    "                            start_p=1, start_q=1,\n",
    "                            test='adf',\n",
    "                            max_p=3, max_q=3, m=1,\n",
    "                            start_P=0, seasonal=False,\n",
    "                            d=None, D=0, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "# Summary of the model\n",
    "print(arima_model.summary())\n",
    "\n",
    "# Forecast\n",
    "forecast, conf_int = arima_model.predict(n_periods=len(test), return_conf_int=True)\n",
    "\n",
    "# Convert results to a DataFrame for visualization or further processing\n",
    "forecast_df = pd.DataFrame(forecast, index=test.index, columns=['forecast'])\n",
    "\n",
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322822f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a50cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
